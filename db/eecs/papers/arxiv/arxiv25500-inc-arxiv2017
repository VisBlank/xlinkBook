arxiv-1702-03920 | Cognitive Mapping and Planning for Visual Navigation | http://arxiv.org/abs/1702.03920 | id:1702.03920 author:Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, Jitendra Malik category:cs.CV cs.AI cs.LG cs.RO  published:2017-02-13 summary:We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person viewpoints and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the planner, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. Our experiments demonstrate that CMP outperforms both reactive strategies and standard memory-based architectures and performs well in novel environments. Furthermore, we show that CMP can also achieve semantically specified goals, such as 'go to a chair'. version:1
arxiv-1702-03877 | Approximate Kernel-based Conditional Independence Tests for Fast Non-Parametric Causal Discovery | http://arxiv.org/abs/1702.03877 | id:1702.03877 author:Eric V. Strobl, Kun Zhang, Shyam Visweswaran category:stat.ME stat.ML  published:2017-02-13 summary:Constraint-based causal discovery (CCD) algorithms require fast and accurate conditional independence (CI) testing. The Kernel Conditional Independence Test (KCIT) is currently one of the most popular CI tests in the non-parametric setting, but many investigators cannot use KCIT with large datasets because the test scales cubicly with sample size. We therefore devise two relaxations called the Randomized Conditional Independence Test (RCIT) and the Randomized conditional Correlation Test (RCoT) which both approximate KCIT by utilizing random Fourier features. In practice, both of the proposed tests scale linearly with sample size and return accurate p-values much faster than KCIT in the large sample size context. CCD algorithms run with RCIT or RCoT also return graphs at least as accurate as the same algorithms run with KCIT but with large reductions in run time. version:1
arxiv-1701-09175 | Skip Connections as Effective Symmetry-Breaking | http://arxiv.org/abs/1701.09175 | id:1701.09175 author:A. Emin Orhan category:cs.NE cs.LG  published:2017-01-31 summary:Skip connections made the training of very deep neural networks possible and have become an indispendable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep neural networks. We argue that skip connections help break symmetries inherent in the loss landscapes of deep networks, leading to drastically simplified landscapes. In particular, skip connections between adjacent layers in a multilayer network break the permutation symmetry of nodes in a given layer, and the recently proposed DenseNet architecture, where each layer projects skip connections to every layer above it, also breaks the rescaling symmetry of connectivity matrices between different layers. This hypothesis is supported by evidence from a toy model with binary weights and from experiments with fully-connected networks suggesting (i) that skip connections do not necessarily improve training unless they help break symmetries and (ii) that alternative ways of breaking the symmetries also lead to significant performance improvements in training deep networks, hence there is nothing special about skip connections in this respect. We find, however, that skip connections confer additional benefits over and above symmetry-breaking, such as the ability to deal effectively with the vanishing gradients problem. version:4
arxiv-1702-03865 | Next-Step Conditioned Deep Convolutional Neural Networks Improve Protein Secondary Structure Prediction | http://arxiv.org/abs/1702.03865 | id:1702.03865 author:Akosua Busia, Navdeep Jaitly category:cs.LG q-bio.BM  published:2017-02-13 summary:Recently developed deep learning techniques have significantly improved the accuracy of various speech and image recognition systems. In this paper we show how to adapt some of these techniques to create a novel chained convolutional architecture with next-step conditioning for improving performance on protein sequence prediction problems. We explore its value by demonstrating its ability to improve performance on eight-class secondary structure prediction. We first establish a state-of-the-art baseline by adapting recent advances in convolutional neural networks which were developed for vision tasks. This model achieves 70.0% per amino acid accuracy on the CB513 benchmark dataset without use of standard performance-boosting techniques such as ensembling or multitask learning. We then improve upon this state-of-the-art result using a novel chained prediction approach which frames the secondary structure prediction as a next-step prediction problem. This sequential model achieves 70.3% Q8 accuracy on CB513 with a single model; an ensemble of these models produces 71.4% Q8 accuracy on the same test set, improving upon the previous overall state of the art for the eight-class secondary structure problem. Our models are implemented using TensorFlow, an open-source machine learning software library available at TensorFlow.org; we aim to release the code for these experiments as part of the TensorFlow repository. version:1
arxiv-1702-03859 | Offline bilingual word vectors, orthogonal transformations and the inverted softmax | http://arxiv.org/abs/1702.03859 | id:1702.03859 author:Samuel L. Smith, David H. P. Turban, Steven Hamblin, Nils Y. Hammerla category:cs.CL cs.AI cs.IR  published:2017-02-13 summary:Usually bilingual word vectors are trained "online". Mikolov et al. showed they can also be found "offline", whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel "inverted softmax" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a "pseudo-dictionary" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%. version:1
arxiv-1702-03856 | Towards speech-to-text translation without speech recognition | http://arxiv.org/abs/1702.03856 | id:1702.03856 author:Sameer Bansal, Herman Kamper, Adam Lopez, Sharon Goldwater category:cs.CL  published:2017-02-13 summary:We explore the problem of translating speech to text in low-resource scenarios where neither automatic speech recognition (ASR) nor machine translation (MT) are available, but we have training data in the form of audio paired with text translations. We present the first system for this problem applied to a realistic multi-speaker dataset, the CALLHOME Spanish-English speech translation corpus. Our approach uses unsupervised term discovery (UTD) to cluster repeated patterns in the audio, creating a pseudotext, which we pair with translations to create a parallel text and train a simple bag-of-words MT model. We identify the challenges faced by the system, finding that the difficulty of cross-speaker UTD results in low recall, but that our system is still able to correctly translate some content words in test data. version:1
arxiv-1702-03849 | Non-convex learning via Stochastic Gradient Langevin Dynamics: a nonasymptotic analysis | http://arxiv.org/abs/1702.03849 | id:1702.03849 author:Maxim Raginsky, Alexander Rakhlin, Matus Telgarsky category:cs.LG math.OC math.PR stat.ML  published:2017-02-13 summary:Stochastic Gradient Langevin Dynamics (SGLD) is a popular variant of Stochastic Gradient Descent, where properly scaled isotropic Gaussian noise is added to an unbiased estimate of the gradient at each iteration. This modest change allows SGLD to escape local minima and suffices to guarantee asymptotic convergence to global minimizers for sufficiently regular non-convex objectives (Gelfand and Mitter, 1991). The present work provides a nonasymptotic analysis in the context of non-convex learning problems: SGLD requires $\tilde{O}(\varepsilon^{-4})$ iterations to sample $\tilde{O}(\varepsilon)$-approximate minimizers of both empirical and population risk, where $\tilde{O}(\cdot)$ hides polynomial dependence on a temperature parameter, the model dimension, and a certain spectral gap parameter. As in the asymptotic setting, our analysis relates the discrete-time SGLD Markov chain to a continuous-time diffusion process. A new tool that drives the results is the use of weighted transportation cost inequalities to quantify the rate of convergence of SGLD to a stationary distribution in the Euclidean $2$-Wasserstein distance. version:1
arxiv-1702-03833 | Estimation of the volume of the left ventricle from MRI images using deep neural networks | http://arxiv.org/abs/1702.03833 | id:1702.03833 author:Fangzhou Liao, Xi Chen, Xiaolin Hu, Sen Song category:cs.CV  published:2017-02-13 summary:Segmenting human left ventricle (LV) in magnetic resonance imaging (MRI) images and calculating its volume are important for diagnosing cardiac diseases. In 2016, Kaggle organized a competition to estimate the volume of LV from MRI images. The dataset consisted of a large number of cases, but only provided systole and diastole volumes as labels. We designed a system based on neural networks to solve this problem. It began with a detector combined with a neural network classifier for detecting regions of interest (ROIs) containing LV chambers. Then a deep neural network named hypercolumns fully convolutional network was used to segment LV in ROIs. The 2D segmentation results were integrated across different images to estimate the volume. With ground-truth volume labels, this model was trained end-to-end. To improve the result, an additional dataset with only segmentation label was used. The model was trained alternately on these two datasets with different types of teaching signals. We also proposed a variance estimation method for the final prediction. Our algorithm ranked the 4th on the test set in this competition. version:1
arxiv-1702-03814 | Bilateral Multi-Perspective Matching for Natural Language Sentences | http://arxiv.org/abs/1702.03814 | id:1702.03814 author:Zhiguo Wang, Wael Hamza, Radu Florian category:cs.AI cs.CL  published:2017-02-13 summary:Natural language sentence matching is a fundamental technology for a variety of tasks. Previous approaches either match sentences from a single direction or only apply single granular (word-by-word or sentence-by-sentence) matching. In this work, we propose a bilateral multi-perspective matching (BiMPM) model under the "matching-aggregation" framework. Given two sentences $P$ and $Q$, our model first encodes them with a BiLSTM encoder. Next, we match the two encoded sentences in two directions $P \rightarrow Q$ and $P \leftarrow Q$. In each matching direction, each time step of one sentence is matched against all time-steps of the other sentence from multiple perspectives. Then, another BiLSTM layer is utilized to aggregate the matching results into a fix-length matching vector. Finally, based on the matching vector, the decision is made through a fully connected layer. We evaluate our model on three tasks: paraphrase identification, natural language inference and answer sentence selection. Experimental results on standard benchmark datasets show that our model achieves the state-of-the-art performance on all tasks. version:1
arxiv-1702-03791 | DNN Filter Bank Cepstral Coefficients for Spoofing Detection | http://arxiv.org/abs/1702.03791 | id:1702.03791 author:Hong Yu, Zheng-Hua Tan, Zhanyu Ma, Jun Guo category:cs.SD cs.CR cs.LG  published:2017-02-13 summary:With the development of speech synthesis techniques, automatic speaker verification systems face the serious challenge of spoofing attack. In order to improve the reliability of speaker verification systems, we develop a new filter bank based cepstral feature, deep neural network filter bank cepstral coefficients (DNN-FBCC), to distinguish between natural and spoofed speech. The deep neural network filter bank is automatically generated by training a filter bank neural network (FBNN) using natural and synthetic speech. By adding restrictions on the training rules, the learned weight matrix of FBNN is band-limited and sorted by frequency, similar to the normal filter bank. Unlike the manually designed filter bank, the learned filter bank has different filter shapes in different channels, which can capture the differences between natural and synthetic speech more effectively. The experimental results on the ASVspoof {2015} database show that the Gaussian mixture model maximum-likelihood (GMM-ML) classifier trained by the new feature performs better than the state-of-the-art linear frequency cepstral coefficients (LFCC) based classifier, especially on detecting unknown attacks. version:1
arxiv-1702-03767 | Is Big Data Sufficient for a Reliable Detection of Non-Technical Losses? | http://arxiv.org/abs/1702.03767 | id:1702.03767 author:Patrick Glauner, Angelo Migliosi, Jorge Meira, Eric Aislan Antonelo, Petko Valtchev, Radu State, Franck Bettinger category:cs.LG cs.AI  published:2017-02-13 summary:Non-technical losses (NTL) occur during the distribution of electricity in power grids and include, but are not limited to, electricity theft and faulty meters. In emerging countries, they may range up to 40% of the total electricity distributed. In order to detect NTLs, machine learning methods are used that learn irregular consumption patterns from customer data and inspection results. The Big Data paradigm followed in modern machine learning reflects the desire of deriving better conclusions from simply analyzing more data, without the necessity of looking at theory and models. However, the sample of inspected customers may be biased, i.e. it does not represent the population of all customers. As a consequence, machine learning models trained on these inspection results are biased as well and therefore lead to unreliable predictions of whether customers cause NTL or not. In machine learning, this issue is called covariate shift and has not been addressed in the literature on NTL detection yet. In this work, we present a novel framework for quantifying and visualizing covariate shift. We apply it to a commercial data set from Brazil that consists of 3.6M customers and 820K inspection results. We show that some features have a stronger covariate shift than others, making predictions less reliable. In particular, previous inspections were focused on certain neighborhoods or customer classes and that they were not sufficiently spread among the population of customers. This framework is about to be deployed in a commercial product for NTL detection. version:1
arxiv-1702-03180 | Stochastic Configuration Networks: Fundamentals and Algorithms | http://arxiv.org/abs/1702.03180 | id:1702.03180 author:Dianhui Wang, Ming Li category:cs.NE  published:2017-02-10 summary:This paper contributes to a development of randomized methods for neural networks. The proposed learner model is generated incrementally by stochastic configuration (SC) algorithms, termed as Stochastic Configuration Networks (SCNs). In contrast to the existing randomised learning algorithms for single layer feed-forward neural networks (SLFNNs), we randomly assign the input weights and biases of the hidden nodes in the light of a supervisory mechanism, and the output weights are analytically evaluated in either constructive or selective manner. As fundamentals of SCN-based data modelling techniques, we establish some theoretical results on the universal approximation property. Three versions of SC algorithms are presented for regression problems (applicable for classification problems as well) in this work. Simulation results concerning both function approximation and real world data regression indicate some remarkable merits of our proposed SCNs in terms of less human intervention on the network size setting, the scope adaptation of random parameters, fast learning and sound generalization. version:2
arxiv-1702-03713 | Feature Space Modeling Through Surrogate Illumination | http://arxiv.org/abs/1702.03713 | id:1702.03713 author:Adam Gaier, Alexander Asteroth, Jean-Baptiste Mouret category:cs.NE cs.CE stat.ML  published:2017-02-13 summary:The MAP-Elites algorithm produces a set of high-performing solutions that vary according to features defined by the user. This technique has the potential to be a powerful tool for design space exploration, but is limited by the need for numerous evaluations. The Surrogate-Assisted Illumination algorithm (SAIL), introduced here, integrates approximative models and intelligent sampling of the objective function to minimize the number of evaluations required by MAP-Elites. The ability of SAIL to efficiently produce both accurate models and diverse high performing solutions is illustrated on a 2D airfoil design problem. The search space is divided into bins, each holding a design with a different combination of features. In each bin SAIL produces a better performing solution than MAP-Elites, and requires several orders of magnitude fewer evaluations. The CMA-ES algorithm was used to produce an optimal design in each bin: with the same number of evaluations required by CMA-ES to find a near-optimal solution in a single bin, SAIL finds solutions of similar quality in every bin. version:1
arxiv-1702-03706 | Multitask Learning with Deep Neural Networks for Community Question Answering | http://arxiv.org/abs/1702.03706 | id:1702.03706 author:Daniele Bonadiman, Antonio Uva, Alessandro Moschitti category:cs.CL  published:2017-02-13 summary:In this paper, we developed a deep neural network (DNN) that learns to solve simultaneously the three tasks of the cQA challenge proposed by the SemEval-2016 Task 3, i.e., question-comment similarity, question-question similarity and new question-comment similarity. The latter is the main task, which can exploit the previous two for achieving better results. Our DNN is trained jointly on all the three cQA tasks and learns to encode questions and comments into a single vector representation shared across the multiple tasks. The results on the official challenge test set show that our approach produces higher accuracy and faster convergence rates than the individual neural networks. Additionally, our method, which does not use any manual feature engineering, approaches the state of the art established with methods that make heavy use of it. version:1
arxiv-1702-03690 | An Efficient Decomposition Framework for Discriminative Segmentation with Supermodular Losses | http://arxiv.org/abs/1702.03690 | id:1702.03690 author:Jiaqian Yu, Matthew B. Blaschko category:cs.CV  published:2017-02-13 summary:Several supermodular losses have been shown to improve the perceptual quality of image segmentation in a discriminative framework such as a structured output support vector machine (SVM). These loss functions do not necessarily have the same structure as the one used by the segmentation inference algorithm, and in general, we may have to resort to generic submodular minimization algorithms for loss augmented inference. Although these come with polynomial time guarantees, they are not practical to apply to image scale data. Many supermodular losses come with strong optimization guarantees, but are not readily incorporated in a loss augmented graph cuts procedure. This motivates our strategy of employing the alternating direction method of multipliers (ADMM) decomposition for loss augmented inference. In doing so, we create a new API for the structured SVM that separates the maximum a posteriori (MAP) inference of the model from the loss augmentation during training. In this way, we gain computational efficiency, making new choices of loss functions practical for the first time, while simultaneously making the inference algorithm employed during training closer to the test time procedure. We show improvement both in accuracy and computational performance on the Microsoft Research Grabcut database and a brain structure segmentation task, empirically validating the use of several supermodular loss functions during training, and the improved computational properties of the proposed ADMM approach over the Fujishige-Wolfe minimum norm point algorithm. version:1
arxiv-1702-03684 | Unsupervised temporal context learning using convolutional neural networks for laparoscopic workflow analysis | http://arxiv.org/abs/1702.03684 | id:1702.03684 author:Sebastian Bodenstedt, Martin Wagner, Darko Katić, Patrick Mietkowski, Benjamin Mayer, Hannes Kenngott, Beat Müller-Stich, Rüdiger Dillmann, Stefanie Speidel category:cs.CV  published:2017-02-13 summary:Computer-assisted surgery (CAS) aims to provide the surgeon with the right type of assistance at the right moment. Such assistance systems are especially relevant in laparoscopic surgery, where CAS can alleviate some of the drawbacks that surgeons incur. For many assistance functions, e.g. displaying the location of a tumor at the appropriate time or suggesting what instruments to prepare next, analyzing the surgical workflow is a prerequisite. Since laparoscopic interventions are performed via endoscope, the video signal is an obvious sensor modality to rely on for workflow analysis. Image-based workflow analysis tasks in laparoscopy, such as phase recognition, skill assessment, video indexing or automatic annotation, require a temporal distinction between video frames. Generally computer vision based methods that generalize from previously seen data are used. For training such methods, large amounts of annotated data are necessary. Annotating surgical data requires expert knowledge, therefore collecting a sufficient amount of data is difficult, time-consuming and not always feasible. In this paper, we address this problem by presenting an unsupervised method for training a convolutional neural network (CNN) to differentiate between laparoscopic video frames on a temporal basis. We extract video frames at regular intervals from 324 unlabeled laparoscopic interventions, resulting in a dataset of approximately 2.2 million images. From this dataset, we extract image pairs from the same video and train a CNN to determine their temporal order. To solve this problem, the CNN has to extract features that are relevant for comprehending laparoscopic workflow. Furthermore, we demonstrate that such a CNN can be adapted for surgical workflow segmentation. We performed image-based workflow segmentation on a publicly available dataset of 7 cholecystectomies and 9 colorectal interventions. version:1
arxiv-1702-03654 | A Morphology-aware Network for Morphological Disambiguation | http://arxiv.org/abs/1702.03654 | id:1702.03654 author:Eray Yildiz, Caglar Tirkaz, H. Bahadir Sahin, Mustafa Tolga Eren, Ozan Sonmez category:cs.CL  published:2017-02-13 summary:Agglutinative languages such as Turkish, Finnish and Hungarian require morphological disambiguation before further processing due to the complex morphology of words. A morphological disambiguator is used to select the correct morphological analysis of a word. Morphological disambiguation is important because it generally is one of the first steps of natural language processing and its performance affects subsequent analyses. In this paper, we propose a system that uses deep learning techniques for morphological disambiguation. Many of the state-of-the-art results in computer vision, speech recognition and natural language processing have been obtained through deep learning models. However, applying deep learning techniques to morphologically rich languages is not well studied. In this work, while we focus on Turkish morphological disambiguation we also present results for French and German in order to show that the proposed architecture achieves high accuracy with no language-specific feature engineering or additional resource. In the experiments, we achieve 84.12, 88.35 and 93.78 morphological disambiguation accuracy among the ambiguous words for Turkish, German and French respectively. version:1
arxiv-1702-03644 | Coresets for Kernel Regression | http://arxiv.org/abs/1702.03644 | id:1702.03644 author:Yan Zheng, Jeff M. Phillips category:cs.LG cs.DS  published:2017-02-13 summary:Kernel regression is an essential and ubiquitous tool for non-parametric data analysis, particularly popular among time series and spatial data. However, the central operation which is performed many times, evaluating a kernel on the data set, takes linear time. This is impractical for modern large data sets. In this paper we describe coresets for kernel regression: compressed data sets which can be used as proxy for the original data and have provably bounded worst case error. The size of the coresets are independent of the raw number of data points, rather they only depend on the error guarantee, and in some cases the size of domain and amount of smoothing. We evaluate our methods on very large time series and spatial data, and demonstrate that they incur negligible error, can be constructed extremely efficiently, and allow for great computational gains. version:1
arxiv-1702-03614 | Multitask diffusion adaptation over networks with common latent representations | http://arxiv.org/abs/1702.03614 | id:1702.03614 author:Jie Chen, Cédric Richard, Ali H. Sayed category:cs.MA stat.ML  published:2017-02-13 summary:Online learning with streaming data in a distributed and collaborative manner can be useful in a wide range of applications. This topic has been receiving considerable attention in recent years with emphasis on both single-task and multitask scenarios. In single-task adaptation, agents cooperate to track an objective of common interest, while in multitask adaptation agents track multiple objectives simultaneously. Regularization is one useful technique to promote and exploit similarity among tasks in the latter scenario. This work examines an alternative way to model relations among tasks by assuming that they all share a common latent feature representation. As a result, a new multitask learning formulation is presented and algorithms are developed for its solution in a distributed online manner. We present a unified framework to analyze the mean-square-error performance of the adaptive strategies, and conduct simulations to illustrate the theoretical findings and potential applications. version:1
arxiv-1702-03613 | A Multi-model Combination Approach for Probabilistic Wind Power Forecasting | http://arxiv.org/abs/1702.03613 | id:1702.03613 author:You Lin, Ming Yang, Can Wan, Jianhui Wang, Yonghua Song category:cs.LG stat.AP  published:2017-02-13 summary:Short-term probabilistic wind power forecasting can provide critical quantified uncertainty information of wind generation for power system operation and control. As the complicated characteristics of wind power prediction error, it would be difficult to develop a universal forecasting model dominating over other alternative models. Therefore, a novel multi-model combination (MMC) approach for short-term probabilistic wind generation forecasting is proposed in this paper to exploit the advantages of different forecasting models. The proposed approach can combine different forecasting models those provide different kinds of probability density functions to improve the probabilistic forecast accuracy. Three probabilistic forecasting models based on the sparse Bayesian learning, kernel density estimation and beta distribution fitting are used to form the combined model. The parameters of the MMC model are solved based on Bayesian framework. Numerical tests illustrate the effectiveness of the proposed MMC approach. version:1
arxiv-1701-07926 | Boosting hazard regression with time-varying covariates | http://arxiv.org/abs/1701.07926 | id:1701.07926 author:Donald K. K. Lee, Ningyuan Chen category:stat.ML 62N02  62G05  published:2017-01-27 summary:Consider a left-truncated right-censored survival process whose evolution depends on time-varying covariates. Given functional data samples from the process, we propose a gradient boosting procedure for estimating its log-intensity function in a flexible manner to capture time-covariate interactions. The estimator is shown to be consistent if the model is correctly specified. Alternatively an oracle inequality can be demonstrated for tree-based models. We use the procedure to shed new light on a question from the operations literature concerning the effect of workload on service rates in an emergency department. To avoid overfitting, boosting employs several regularization devices. One of them is step-size restriction, but the rationale for this is somewhat mysterious from the viewpoint of consistency: In theoretical treatments of classification and regression problems, unrestricted greedy step-sizes appear to suffice. Given that the partial log-likelihood functional for hazard regression has unbounded curvature, our study suggests that step-size restriction might be a mechanism for preventing the curvature of the risk from derailing convergence. version:2
arxiv-1702-03605 | Nearly Instance Optimal Sample Complexity Bounds for Top-k Arm Selection | http://arxiv.org/abs/1702.03605 | id:1702.03605 author:Lijie Chen, Jian Li, Mingda Qiao category:cs.LG cs.DS  published:2017-02-13 summary:In the Best-$k$-Arm problem, we are given $n$ stochastic bandit arms, each associated with an unknown reward distribution. We are required to identify the $k$ arms with the largest means by taking as few samples as possible. In this paper, we make progress towards a complete characterization of the instance-wise sample complexity bounds for the Best-$k$-Arm problem. On the lower bound side, we obtain a novel complexity term to measure the sample complexity that every Best-$k$-Arm instance requires. This is derived by an interesting and nontrivial reduction from the Best-$1$-Arm problem. We also provide an elimination-based algorithm that matches the instance-wise lower bound within doubly-logarithmic factors. The sample complexity of our algorithm strictly dominates the state-of-the-art for Best-$k$-Arm (module constant factors). version:1
arxiv-1702-03600 | Underwater Optical Image Processing: A Comprehensive Review | http://arxiv.org/abs/1702.03600 | id:1702.03600 author:Huimin Lu, Yujie Li, Yudong Zhang, Min Chen, Seiichi Serikawa, Hyoungseop Kim category:cs.CV 78  published:2017-02-13 summary:Underwater cameras are widely used to observe the sea floor. They are usually included in autonomous underwater vehicles, unmanned underwater vehicles, and in situ ocean sensor networks. Despite being an important sensor for monitoring underwater scenes, there exist many issues with recent underwater camera sensors. Because of lights transportation characteristics in water and the biological activity at the sea floor, the acquired underwater images often suffer from scatters and large amounts of noise. Over the last five years, many methods have been proposed to overcome traditional underwater imaging problems. This paper aims to review the state-of-the-art techniques in underwater image processing by highlighting the contributions and challenges presented in over 40 papers. We present an overview of various underwater image processing approaches, such as underwater image descattering, underwater image color restoration, and underwater image quality assessments. Finally, we summarize the future trends and challenges in designing and processing underwater imaging sensors. version:1
arxiv-1702-03584 | Similarity Preserving Representation Learning for Time Series Analysis | http://arxiv.org/abs/1702.03584 | id:1702.03584 author:Qi Lei, Jinfeng Yi, Roman Vaculin, Lingfei Wu, Inderjit S. Dhillon category:cs.AI cs.LG  published:2017-02-12 summary:A considerable amount of machine learning algorithms take matrices as their inputs. As such, they cannot directly analyze time series data due to its temporal nature, usually unequal lengths, and complex properties. This is a great pity since many of these algorithms are effective, robust, efficient, and easy to use. In this paper, we bridge this gap by proposing an efficient representation learning framework that is able to convert a set of time series with equal or unequal lengths to a matrix format. In particular, we guarantee that the pairwise similarities between time series are well preserved after the transformation. Therefore, the learned feature representation is particularly suitable to the class of learning problems that are sensitive to data similarities. Given a set of $n$ time series, we first construct an $n\times n$ partially observed similarity matrix by randomly sampling $O(n \log n)$ pairs of time series and computing their pairwise similarities. We then propose an extremely efficient algorithm that solves a highly non-convex and NP-hard problem to learn new features based on the partially observed similarity matrix. We use the learned features to conduct experiments on both data classification and clustering tasks. Our extensive experimental results demonstrate that the proposed framework is both effective and efficient. version:1
arxiv-1702-03537 | Supervised Learning for Controlled Dynamical System Learning | http://arxiv.org/abs/1702.03537 | id:1702.03537 author:Ahmed Hefny, Carlton Downey, Geoffrey J. Gordon category:stat.ML  published:2017-02-12 summary:We develop a framework for reducing the identification of controlled dynamical systems to solving a small set of supervised learning problems. We do this by adapting the two-stage regression framework proposed in (Hefny et. al. 2015) to controlled systems, which are more subtle than uncontrolled systems since they require a state representation that tolerates changes in the action policy. We then use the proposed framework to develop a non-parametric controlled system identification method that approximates the Hilbert-Space Embedding of a PSR (HSE-PSR) using random Fourier features, resulting in significant gains in learning speed. We also propose an iterative procedure for improving model parameters given an initial estimate. We report promising results on multiple experiments. version:1
arxiv-1702-03525 | Learning to Parse and Translate Improves Neural Machine Translation | http://arxiv.org/abs/1702.03525 | id:1702.03525 author:Akiko Eriguchi, Yoshimasa Tsuruoka, Kyunghyun Cho category:cs.CL  published:2017-02-12 summary:There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RG. version:1
arxiv-1702-03522 | Spectral Clustering via Graph Filtering: Consistency on the High-Dimensional Stochastic Block Model | http://arxiv.org/abs/1702.03522 | id:1702.03522 author:Muni Sreenivas Pydi, Ambedkar Dukkipati category:stat.ML  published:2017-02-12 summary:Spectral clustering is amongst the most popular methods for community detection in graphs. A key step in spectral clustering algorithms is the eigen-decomposition of the $n{\times}n$ graph Laplacian matrix to extract its $k$ leading eigenvectors, where $k$ is the desired number of clusters among $n$ objects. This is prohibitively complex to implement for very large datasets. However, it has recently been shown that it is possible to bypass the eigen-decomposition by computing an approximate spectral embedding through graph filtering of random signals. In this paper, we prove that spectral clustering performed via graph filtering can still recover the planted clusters consistently, under mild conditions. We analyse the effects of sparsity, dimensionality and filter approximation error on the consistency of the algorithm. version:1
arxiv-1702-03515 | Sparse Representation based Multi-sensor Image Fusion: A Review | http://arxiv.org/abs/1702.03515 | id:1702.03515 author:Qiang Zhang, Yi Liu, Rick S. Blum, Jungong Han, Dacheng Tao category:cs.CV  published:2017-02-12 summary:As a result of several successful applications in computer vision and image processing, sparse representation (SR) has attracted significant attention in multi-sensor image fusion. Unlike the traditional multiscale transforms (MSTs) that presume the basis functions, SR learns an over-complete dictionary from a set of training images for image fusion, and it achieves more stable and meaningful representations of the source images. By doing so, the SR-based fusion methods generally outperform the traditional MST-based image fusion methods in both subjective and objective tests. In addition, they are less susceptible to mis-registration among the source images, thus facilitating the practical applications. This survey paper proposes a systematic review of the SR-based multi-sensor image fusion literature, highlighting the pros and cons of each category of approaches. Specifically, we start by performing a theoretical investigation of the entire system from three key algorithmic aspects, (1) sparse representation models; (2) dictionary learning methods; and (3) activity levels and fusion rules. Subsequently, we show how the existing works address these scientific problems and design the appropriate fusion rules for each application, such as multi-focus image fusion and multi-modality (e.g., infrared and visible) image fusion. At last, we carry out some experiments to evaluate the impact of these three algorithmic components on the fusion performance when dealing with different applications. This article is expected to serve as a tutorial and source of reference for researchers preparing to enter the field or who desire to employ the sparse representation theory in other fields. version:1
arxiv-1702-03505 | A Novel Weight-Shared Multi-Stage Network Architecture of CNNs for Scale Invariance | http://arxiv.org/abs/1702.03505 | id:1702.03505 author:Ryo Takahashi, Takashi Matsubara, Kuniaki Uehara category:cs.CV  published:2017-02-12 summary:Convolutional neural networks (CNNs) have demonstrated remarkable results in image classification tasks for benchmark and practical uses. The CNNs with deeper architectures have achieved higher performances thanks to their numerous parameters and resulting high expression ability recently. However, the CNNs have a problem of limited robustness to geometric transformation of objects in images such as scaling and rotation. This problem is considered to limit performance improvement of the deep CNNs but there is no established solution. This study focuses on scale transformation and proposes a novel network architecture called weight-shared multi-stage network (WSMS-Net), which enables the existing deep CNNs, such as ResNet and DenseNet, to acquire robustness to scaling of objects. The WSMS-Net architecture consists of multiple stages of CNNs and is easily combined with existing deep CNNs. This study demonstrates that existing deep CNNs combined the proposed WSMS-Net archive higher accuracy for image classification tasks only with little increase in the number of parameters. version:1
arxiv-1702-03500 | Concept Drift Adaptation by Exploiting Historical Knowledge | http://arxiv.org/abs/1702.03500 | id:1702.03500 author:Yu Sun, Ke Tang, Zexuan Zhu, Xin Yao category:cs.LG  published:2017-02-12 summary:Incremental learning with concept drift has often been tackled by ensemble methods, where models built in the past can be re-trained to attain new models for the current data. Two design questions need to be addressed in developing ensemble methods for incremental learning with concept drift, i.e., which historical (i.e., previously trained) models should be preserved and how to utilize them. A novel ensemble learning method, namely Diversity and Transfer based Ensemble Learning (DTEL), is proposed in this paper. Given newly arrived data, DTEL uses each preserved historical model as an initial model and further trains it with the new data via transfer learning. Furthermore, DTEL preserves a diverse set of historical models, rather than a set of historical models that are merely accurate in terms of classification accuracy. Empirical studies on 15 synthetic data streams and 4 real-world data streams (all with concept drifts) demonstrate that DTEL can handle concept drift more effectively than 4 other state-of-the-art methods. version:1
arxiv-1702-03470 | Vector Embedding of Wikipedia Concepts and Entities | http://arxiv.org/abs/1702.03470 | id:1702.03470 author:Ehsan Sherkat, Evangelos Milios category:cs.CL  published:2017-02-12 summary:Using deep learning for different machine learning tasks such as image classification and word embedding has recently gained many attentions. Its appealing performance reported across specific Natural Language Processing (NLP) tasks in comparison with other approaches is the reason for its popularity. Word embedding is the task of mapping words or phrases to a low dimensional numerical vector. In this paper, we use deep learning to embed Wikipedia Concepts and Entities. The English version of Wikipedia contains more than five million pages, which suggest its capability to cover many English Entities, Phrases, and Concepts. Each Wikipedia page is considered as a concept. Some concepts correspond to entities, such as a person's name, an organization or a place. Contrary to word embedding, Wikipedia Concepts Embedding is not ambiguous, so there are different vectors for concepts with similar surface form but different mentions. We proposed several approaches and evaluated their performance based on Concept Analogy and Concept Similarity tasks. The results show that proposed approaches have the performance comparable and in some cases even higher than the state-of-the-art methods. version:1
arxiv-1702-03465 | Enabling Robots to Communicate their Objectives | http://arxiv.org/abs/1702.03465 | id:1702.03465 author:Sandy H. Huang, David Held, Pieter Abbeel, Anca D. Dragan category:cs.RO cs.LG  published:2017-02-11 summary:Our ultimate goal is to efficiently enable end-users to correctly anticipate a robot's behavior in novel situations. This behavior is often a direct result of the robot's underlying objective function. Our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, and then selecting those behaviors that are maximally informative. The problem of computing a posterior over objectives from observed behavior is known as Inverse Reinforcement Learning (IRL), and has been applied to robots learning human objectives. We consider the problem where the roles of human and robot are swapped. Our main contribution is to recognize that unlike robots, humans will not be \emph{exact} in their IRL inference. We thus introduce two factors to define candidate approximate-inference models for human learning in this setting, and analyze them in a user study in the autonomous driving domain. We show that certain approximate-inference models lead to the robot generating example behaviors that better enable users to anticipate what the robot will do in test situations. Our results also suggest, however, that additional research is needed in modeling how humans extrapolate from examples of robot behavior. version:1
arxiv-1702-03464 | Gromov-Hausdorff limit of Wasserstein spaces on point clouds | http://arxiv.org/abs/1702.03464 | id:1702.03464 author:Nicolas Garcia Trillos category:math.MG math.AP math.PR math.ST stat.ML stat.TH  published:2017-02-11 summary:We consider a point cloud $X_n := \{ x_1, \dots, x_n \}$ uniformly distributed on the flat torus $\mathbb{T}^d : = \mathbb{R}^d / \mathbb{Z}^d $, and construct a geometric graph on the cloud by connecting points that are within distance $\epsilon$ of each other. We let $\mathcal{P}(X_n)$ be the space of probability measures on $X_n$ and endow it with a discrete Wasserstein distance $W_n$ as defined by Maas. We show that as long as $\epsilon= \epsilon_n$ decays towards zero slower than an explicit rate depending on the level of uniformity of $X_n$, then the space $(\mathcal{P}(X_n), W_n)$ converges in the Gromov-Hausdorff sense towards the space of probability measures on $\mathbb{T}^d$ endowed with the Wasserstein distance. version:1
arxiv-1702-03447 | A Collective, Probabilistic Approach to Schema Mapping: Appendix | http://arxiv.org/abs/1702.03447 | id:1702.03447 author:Angelika Kimmig, Alex Memory, Renee J. Miller, Lise Getoor category:cs.DB cs.LG  published:2017-02-11 summary:In this appendix we provide additional supplementary material to "A Collective, Probabilistic Approach to Schema Mapping." We include an additional extended example, supplementary experiment details, and proof for the complexity result stated in the main paper. version:1
arxiv-1702-03446 | On the Global-Local Dichotomy in Sparsity Modeling | http://arxiv.org/abs/1702.03446 | id:1702.03446 author:Dmitry Batenkov, Yaniv Romano, Michael Elad category:cs.IT math.IT stat.ML  published:2017-02-11 summary:The traditional sparse modeling approach, when applied to inverse problems with large data such as images, essentially assumes a sparse model for small overlapping data patches. While producing state-of-the-art results, this methodology is suboptimal, as it does not attempt to model the entire global signal in any meaningful way - a nontrivial task by itself. In this paper we propose a way to bridge this theoretical gap by constructing a global model from the bottom up. Given local sparsity assumptions in a dictionary, we show that the global signal representation must satisfy a constrained underdetermined system of linear equations, which can be solved efficiently by modern optimization methods such as Alternating Direction Method of Multipliers (ADMM). We investigate conditions for unique and stable recovery, and provide numerical evidence corroborating the theory. version:1
arxiv-1702-03443 | Group Scissor: Scaling Neuromorphic Computing Design to Big Neural Networks | http://arxiv.org/abs/1702.03443 | id:1702.03443 author:Yandan Wang, Wei Wen, Beiye Liu, Donald Chiarulli, Hai Li category:cs.NE cs.AI C.1.3  I.2.6  I.5.1  published:2017-02-11 summary:Synapse crossbar is an elementary structure in Neuromorphic Computing Systems (NCS). However, the limited size of crossbars and heavy routing congestion impedes the NCS implementations of big neural networks. In this paper, we propose a two-step framework (namely, \textit{group scissor}) to scale NCS designs to big neural networks. The first step is \textit{rank clipping}, which integrates low-rank approximation into the training to reduce total crossbar area. The second step is \textit{group connection deletion}, which structurally prunes connections to reduce routing congestion between crossbars. Tested on convolutional neural networks of \textit{LeNet} on MNIST database and \textit{ConvNet} on CIFAR-10 database, our experiments show significant reduction of crossbar area and routing area in NCS designs. Without accuracy loss, rank clipping reduces total crossbar area to 13.62\% and 51.81\% in the NCS designs of \textit{LeNet} and \textit{ConvNet}, respectively. Following rank clipping, group connection deletion further reduces the routing area of \textit{LeNet} and \textit{ConvNet} to 8.1\% and 52.06\%, respectively. version:1
arxiv-1702-03435 | Distributed Mapping with Privacy and Communication Constraints: Lightweight Algorithms and Object-based Models | http://arxiv.org/abs/1702.03435 | id:1702.03435 author:Siddharth Choudhary, Luca Carlone, Carlos Nieto, John Rogers, Henrik I. Christensen, Frank Dellaert category:cs.RO cs.CV  published:2017-02-11 summary:We consider the following problem: a team of robots is deployed in an unknown environment and it has to collaboratively build a map of the area without a reliable infrastructure for communication. The backbone for modern mapping techniques is pose graph optimization, which estimates the trajectory of the robots, from which the map can be easily built. The first contribution of this paper is a set of distributed algorithms for pose graph optimization: rather than sending all sensor data to a remote sensor fusion server, the robots exchange very partial and noisy information to reach an agreement on the pose graph configuration. Our approach can be considered as a distributed implementation of the two-stage approach of Carlone et al., where we use the Successive Over-Relaxation (SOR) and the Jacobi Over-Relaxation (JOR) as workhorses to split the computation among the robots. As a second contribution, we extend %and demonstrate the applicability of the proposed distributed algorithms to work with object-based map models. The use of object-based models avoids the exchange of raw sensor measurements (e.g., point clouds) further reducing the communication burden. Our third contribution is an extensive experimental evaluation of the proposed techniques, including tests in realistic Gazebo simulations and field experiments in a military test facility. Abundant experimental evidence suggests that one of the proposed algorithms (the Distributed Gauss-Seidel method or DGS) has excellent performance. The DGS requires minimal information exchange, has an anytime flavor, scales well to large teams, is robust to noise, and is easy to implement. Our field tests show that the combined use of our distributed algorithms and object-based models reduces the communication requirements by several orders of magnitude and enables distributed mapping with large teams of robots in real-world problems. version:1
arxiv-1702-03431 | Crossing Nets: Dual Generative Models with a Shared Latent Space for Hand Pose Estimation | http://arxiv.org/abs/1702.03431 | id:1702.03431 author:Chengde Wan, Thomas Probst, Luc Van Gool, Angela Yao category:cs.CV  published:2017-02-11 summary:State-of-the-art methods for 3D hand pose estimation from depth images require large amounts of annotated training data. We propose to model the statistical relationships of 3D hand poses and corresponding depth images using two deep generative models with a shared latent space. By design, our architecture allows for learning from unlabeled image data in a semi-supervised manner. Assuming a one-to-one mapping between a pose and a depth map, any given point in the shared latent space can be projected into both a hand pose and a corresponding depth map. Regressing the hand pose can then be done by learning a discriminator to estimate the posterior of the latent pose given some depth map. To improve generalization and to better exploit unlabeled depth maps, we jointly train a generator and a discriminator. At each iteration, the generator is updated with the back-propagated gradient from the discriminator to synthesize realistic depth maps of the articulated hand, while the discriminator benefits from an augmented training set of synthesized and unlabeled samples. The proposed discriminator network architecture is highly efficient and runs at 90 FPS on the CPU with accuracies comparable or better than state-of-art on 3 publicly available benchmarks. version:1
arxiv-1702-03410 | ArtGAN: Artwork Synthesis with Conditional Categorial GANs | http://arxiv.org/abs/1702.03410 | id:1702.03410 author:Wei Ren Tan, Chee Seng Chan, Hernan Aguirre, Kiyoshi Tanaka category:cs.CV  published:2017-02-11 summary:This paper proposes an extension to the Generative Adversarial Networks (GANs), namely as ARTGAN to synthetically generate more challenging and complex images such as artwork that have abstract characteristics. This is in contrast to most of the current solutions that focused on generating natural images such as room interiors, birds, flowers and faces. The key innovation of our work is to allow back-propagation of the loss function w.r.t. the labels (randomly assigned to each generated images) to the generator from the discriminator. With the feedback from the label information, the generator is able to learn faster and achieve better generated image quality. Empirically, we show that the proposed ARTGAN is capable to create realistic artwork, as well as generate compelling real world images that globally look natural with clear shape on CIFAR-10. version:1
arxiv-1702-03407 | Reverse Classification Accuracy: Predicting Segmentation Performance in the Absence of Ground Truth | http://arxiv.org/abs/1702.03407 | id:1702.03407 author:Vanya V. Valindria, Ioannis Lavdas, Wenjia Bai, Konstantinos Kamnitsas, Eric O. Aboagye, Andrea G. Rockall, Daniel Rueckert, Ben Glocker category:cs.CV  published:2017-02-11 summary:When integrating computational tools such as automatic segmentation into clinical practice, it is of utmost importance to be able to assess the level of accuracy on new data, and in particular, to detect when an automatic method fails. However, this is difficult to achieve due to absence of ground truth. Segmentation accuracy on clinical data might be different from what is found through cross-validation because validation data is often used during incremental method development, which can lead to overfitting and unrealistic performance expectations. Before deployment, performance is quantified using different metrics, for which the predicted segmentation is compared to a reference segmentation, often obtained manually by an expert. But little is known about the real performance after deployment when a reference is unavailable. In this paper, we introduce the concept of reverse classification accuracy (RCA) as a framework for predicting the performance of a segmentation method on new data. In RCA we take the predicted segmentation from a new image to train a reverse classifier which is evaluated on a set of reference images with available ground truth. The hypothesis is that if the predicted segmentation is of good quality, then the reverse classifier will perform well on at least some of the reference images. We validate our approach on multi-organ segmentation with different classifiers and segmentation methods. Our results indicate that it is indeed possible to predict the quality of individual segmentations, in the absence of ground truth. Thus, RCA is ideal for integration into automatic processing pipelines in clinical routine and as part of large-scale image analysis studies. version:1
arxiv-1702-03402 | Parallel Long Short-Term Memory for Multi-stream Classification | http://arxiv.org/abs/1702.03402 | id:1702.03402 author:Mohamed Bouaziz, Mohamed Morchid, Richard Dufour, Georges Linarès, Renato De Mori category:cs.CL cs.LG  published:2017-02-11 summary:Recently, machine learning methods have provided a broad spectrum of original and efficient algorithms based on Deep Neural Networks (DNN) to automatically predict an outcome with respect to a sequence of inputs. Recurrent hidden cells allow these DNN-based models to manage long-term dependencies such as Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM). Nevertheless, these RNNs process a single input stream in one (LSTM) or two (Bidirectional LSTM) directions. But most of the information available nowadays is from multistreams or multimedia documents, and require RNNs to process these information synchronously during the training. This paper presents an original LSTM-based architecture, named Parallel LSTM (PLSTM), that carries out multiple parallel synchronized input sequences in order to predict a common output. The proposed PLSTM method could be used for parallel sequence classification purposes. The PLSTM approach is evaluated on an automatic telecast genre sequences classification task and compared with different state-of-the-art architectures. Results show that the proposed PLSTM method outperforms the baseline n-gram models as well as the state-of-the-art LSTM approach. version:1
arxiv-1702-00288 | Low-Dose CT with a Residual Encoder-Decoder Convolutional Neural Network (RED-CNN) | http://arxiv.org/abs/1702.00288 | id:1702.00288 author:Hu Chen, Yi Zhang, Mannudeep K. Kalra, Feng Lin, Peixi Liao, Jiliu Zhou, Ge Wang category:physics.med-ph cs.NE  published:2017-02-01 summary:Given the potential X-ray radiation risk to the patient, low-dose CT has attracted a considerable interest in the medical imaging field. The current main stream low-dose CT methods include vendor-specific sinogram domain filtration and iterative reconstruction, but they need to access original raw data whose formats are not transparent to most users. Due to the difficulty of modeling the statistical characteristics in the image domain, the existing methods for directly processing reconstructed images cannot eliminate image noise very well while keeping structural details. Inspired by the idea of deep learning, here we combine the autoencoder, the deconvolution network, and shortcut connections into the residual encoder-decoder convolutional neural network (RED-CNN) for low-dose CT imaging. After patch-based training, the proposed RED-CNN achieves a competitive performance relative to the-state-of-art methods in both simulated and clinical cases. Especially, our method has been favorably evaluated in terms of noise suppression, structural preservation and lesion detection. version:2
arxiv-1702-03389 | Whale swarm algorithm for function optimization | http://arxiv.org/abs/1702.03389 | id:1702.03389 author:Bing Zeng, Liang Gao, Xinyu Li category:cs.NE  published:2017-02-11 summary:Increasing nature-inspired metaheuristic algorithms are applied to solving the real-world optimization problems, as they have some advantages over the classical methods of numerical optimization. This paper has proposed a new nature-inspired metaheuristic called Whale Swarm Algorithm for function optimization, which is inspired by the whales behavior of communicating with each other via ultrasound for hunting. The proposed Whale Swarm Algorithm has been compared with several popular metaheuristic algorithms on comprehensive performance metrics. According to the experimental results, Whale Swarm Algorithm has a quite competitive performance when compared with other algorithms. version:1
arxiv-1702-03380 | Training Deep Neural Networks via Optimization Over Graphs | http://arxiv.org/abs/1702.03380 | id:1702.03380 author:Guoqiang Zhang, W. Bastiaan Kleijn category:cs.LG cs.DC  published:2017-02-11 summary:In this work, we propose to train a deep neural network by distributed optimization over a graph. Two nonlinear functions are considered: the rectified linear unit (ReLU) and a linear unit with both lower and upper cutoffs (DCutLU). The problem reformulation over a graph is realized by explicitly representing ReLU or DCutLU using a set of slack variables. We then apply the alternating direction method of multipliers (ADMM) to update the weights of the network layerwise by solving subproblems of the reformulated problem. Empirical results suggest that by proper parameter selection, the ADMM- based method converges considerably faster than gradient descent method. version:1
arxiv-1702-02211 | Fixing the Infix: Unsupervised Discovery of Root-and-Pattern Morphology | http://arxiv.org/abs/1702.02211 | id:1702.02211 author:Tarek Sakakini, Suma Bhat, Pramod Viswanath category:cs.CL  published:2017-02-07 summary:We present an unsupervised and language-agnostic method for learning root-and-pattern morphology in Semitic languages. This form of morphology, abundant in Semitic languages, has not been handled in prior unsupervised approaches. We harness the syntactico-semantic information in distributed word representations to solve the long standing problem of root-and-pattern discovery in Semitic languages. Moreover, we construct an unsupervised root extractor based on the learned rules. We prove the validity of learned rules across Arabic, Hebrew, and Amharic, alongside showing that our root extractor compares favorably with a widely used, carefully engineered root extractor: ISRI. version:2
arxiv-1702-02212 | MORSE: Semantic-ally Drive-n MORpheme SEgment-er | http://arxiv.org/abs/1702.02212 | id:1702.02212 author:Tarek Sakakini, Suma Bhat, Pramod Viswanath category:cs.CL  published:2017-02-07 summary:We present in this paper a novel framework for morpheme segmentation which uses the morpho-syntactic regularities preserved by word representations, in addition to orthographic features, to segment words into morphemes. This framework is the first to consider vocabulary-wide syntactico-semantic information for this task. We also analyze the deficiencies of available benchmarking datasets and introduce our own dataset that was created on the basis of compositionality. We validate our algorithm across datasets and present state-of-the-art results. version:2
arxiv-1702-03349 | Enhanced Local Binary Patterns for Automatic Face Recognition | http://arxiv.org/abs/1702.03349 | id:1702.03349 author:Pavel Král, Antonín Vrba category:cs.CV  published:2017-02-10 summary:This paper presents a novel automatic face recognition approach based on local binary patterns (LBP). LBP descriptor considers a local neighbourhood of a pixel to compute the features. This method is not very robust to handle image noise, variances and different illumination conditions. In this paper, we address these issues and extend the original LBP operator by considering more pixels and different neighbourhoods to compute the feature vector. The proposed method is evaluated on two benchmark corpora, namely UFI and FERET face datasets. We experimentally show that our approach is very efficient because it significantly outperforms several other state-of-the-art methods and is efficient particularly in the real conditions where the above mentioned issues are obvious. version:1
arxiv-1702-03345 | Multi-Resolution Dual-Tree Wavelet Scattering Network for Signal Classification | http://arxiv.org/abs/1702.03345 | id:1702.03345 author:Amarjot Singh, Nick Kingsbury category:cs.CV  published:2017-02-10 summary:This paper introduces a Deep Scattering network that utilizes Dual-Tree complex wavelets to extract translation invariant representations from an input signal. The computationally efficient Dual-Tree wavelets decompose the input signal into densely spaced representations over scales. Translation invariance is introduced in the representations by applying a non-linearity over a region followed by averaging. The discriminatory information in the densely spaced, locally smooth, signal representations aids the learning of the classifier. The proposed network is shown to outperform Mallat's ScatterNet on four datasets with different modalities on classification accuracy. version:1
arxiv-1702-03342 | Learning Concept Embeddings for Efficient Bag-of-Concepts Densification | http://arxiv.org/abs/1702.03342 | id:1702.03342 author:Walid Shalaby, Wlodek Zadrozny category:cs.CL  published:2017-02-10 summary:Explicit concept space models have proven efficacy for text representation in many natural language and text mining applications. The idea is to embed textual structures into a semantic space of concepts which captures the main topics of these structures. That so called bag-of-concepts representation suffers from data sparsity causing low similarity scores between similar texts due to low concept overlap. In this paper we propose two neural embedding models in order to learn continuous concept vectors. Once learned, we propose an efficient vector aggregation method to generate fully dense bag-of-concepts representations. Empirical results on a benchmark dataset for measuring entity semantic relatedness show superior performance over other concept embedding models. In addition, by utilizing our efficient aggregation method, we demonstrate the effectiveness of the densified vector representation over the typical sparse representations for dataless classification where we can achieve at least same or better accuracy with much less dimensions. version:1
arxiv-1702-03334 | Batch Policy Gradient Methods for Improving Neural Conversation Models | http://arxiv.org/abs/1702.03334 | id:1702.03334 author:Kirthevasan Kandasamy, Yoram Bachrach, Ryota Tomioka, Daniel Tarlow, David Carter category:stat.ML cs.LG  published:2017-02-10 summary:We study reinforcement learning of chatbots with recurrent neural network architectures when the rewards are noisy and expensive to obtain. For instance, a chatbot used in automated customer service support can be scored by quality assurance agents, but this process can be expensive, time consuming and noisy. Previous reinforcement learning work for natural language processing uses on-policy updates and/or is designed for on-line learning settings. We demonstrate empirically that such strategies are not appropriate for this setting and develop an off-policy batch policy gradient method (BPG). We demonstrate the efficacy of our method via a series of synthetic experiments and an Amazon Mechanical Turk experiment on a restaurant recommendations dataset. version:1
arxiv-1702-03307 | Generative Mixture of Networks | http://arxiv.org/abs/1702.03307 | id:1702.03307 author:Ershad Banijamali, Ali Ghodsi, Pascal Poupart category:cs.LG stat.ML  published:2017-02-10 summary:A generative model based on training deep architectures is proposed. The model consists of K networks that are trained together to learn the underlying distribution of a given data set. The process starts with dividing the input data into K clusters and feeding each of them into a separate network. After few iterations of training networks separately, we use an EM-like algorithm to train the networks together and update the clusters of the data. We call this model Mixture of Networks. The provided model is a platform that can be used for any deep structure and be trained by any conventional objective function for distribution modeling. As the components of the model are neural networks, it has high capability in characterizing complicated data distributions as well as clustering data. We apply the algorithm on MNIST hand-written digits and Yale face datasets. We also demonstrate the clustering ability of the model using some real-world and toy examples. version:1
arxiv-1702-03305 | Universal Dependencies to Logical Forms with Negation Scope | http://arxiv.org/abs/1702.03305 | id:1702.03305 author:Federico Fancellu, Siva Reddy, Adam Lopez, Bonnie Webber category:cs.CL 03B65  published:2017-02-10 summary:Many language technology applications would benefit from the ability to represent negation and its scope on top of widely-used linguistic resources. In this paper, we investigate the possibility of obtaining a first-order logic representation with negation scope marked using Universal Dependencies. To do so, we enhance UDepLambda, a framework that converts dependency graphs to logical forms. The resulting UDepLambda$\lnot$ is able to handle phenomena related to scope by means of an higher-order type theory, relevant not only to negation but also to universal quantification and other complex semantic phenomena. The initial conversion we did for English is promising, in that one can represent the scope of negation also in the presence of more complex phenomena such as universal quantifiers. version:1
arxiv-1702-03275 | Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models | http://arxiv.org/abs/1702.03275 | id:1702.03275 author:Sergey Ioffe category:cs.LG  published:2017-02-10 summary:Batch Normalization is quite effective at accelerating and improving the training of deep models. However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples. We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference. We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch. Models trained with Batch Renormalization perform substantially better than batchnorm when training with small or non-i.i.d. minibatches. At the same time, Batch Renormalization retains the benefits of batchnorm such as insensitivity to initialization and training efficiency. version:1
arxiv-1702-03274 | Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning | http://arxiv.org/abs/1702.03274 | id:1702.03274 author:Jason D. Williams, Kavosh Asadi, Geoffrey Zweig category:cs.AI cs.CL  published:2017-02-10 summary:End-to-end learning of recurrent neural networks (RNNs) is an attractive solution for dialog systems; however, current techniques are data-intensive and require thousands of dialogs to learn simple behaviors. We introduce Hybrid Code Networks (HCNs), which combine an RNN with domain-specific knowledge encoded as software and system action templates. Compared to existing end-to-end approaches, HCNs considerably reduce the amount of training data required, while retaining the key benefit of inferring a latent representation of dialog state. In addition, HCNs can be optimized with supervised learning, reinforcement learning, or a mixture of both. HCNs attain state-of-the-art performance on the bAbI dialog dataset, and outperform two commercially deployed customer-facing dialog systems. version:1
arxiv-1702-03267 | Dual-Tree Wavelet Scattering Network with Parametric Log Transformation for Object Classification | http://arxiv.org/abs/1702.03267 | id:1702.03267 author:Amarjot Singh, Nick Kingsbury category:cs.CV  published:2017-02-10 summary:We introduce a ScatterNet that uses a parametric log transformation with Dual-Tree complex wavelets to extract translation invariant representations from a multi-resolution image. The parametric transformation aids the OLS pruning algorithm by converting the skewed distributions into relatively mean-symmetric distributions while the Dual-Tree wavelets improve the computational efficiency of the network. The proposed network is shown to outperform Mallat's ScatterNet on two image datasets, both for classification accuracy and computational efficiency. The advantages of the proposed network over other supervised and some unsupervised methods are also presented using experiments performed on different training dataset sizes. version:1
arxiv-1702-03260 | A Deterministic and Generalized Framework for Unsupervised Learning with Restricted Boltzmann Machines | http://arxiv.org/abs/1702.03260 | id:1702.03260 author:Eric W. Tramel, Marylou Gabrié, Andre Manoel, Francesco Caltagirone, Florent Krzakala category:cs.LG cond-mat.dis-nn cs.NE stat.ML  published:2017-02-10 summary:Restricted Boltzmann machines (RBMs) are energy-based neural-networks which are commonly used as the building blocks for deep architectures neural architectures. In this work, we derive a deterministic framework for the training, evaluation, and use of RBMs based upon the Thouless-Anderson-Palmer (TAP) mean-field approximation of widely-connected systems with weak interactions coming from spin-glass theory. While the TAP approach has been extensively studied for fully-visible binary spin systems, our construction is generalized to latent-variable models, as well as to arbitrarily distributed real-valued spin systems with bounded support. In our numerical experiments, we demonstrate the effective deterministic training of our proposed models and are able to show interesting features of unsupervised learning which could not be directly observed with sampling. Additionally, we demonstrate how to utilize our TAP-based framework for leveraging trained RBMs as joint priors in denoising problems. version:1
arxiv-1702-03258 | Soft tensegrity robots | http://arxiv.org/abs/1702.03258 | id:1702.03258 author:John Rieffel, Jean-Baptiste Mouret category:cs.RO cs.LG cs.SY  published:2017-02-10 summary:Living organisms intertwine soft (e.g., muscle) and hard (e.g., bones) materials, giving them an intrinsic flexibility and resiliency often lacking in conventional rigid robots. The emerging field of soft robotics seeks to harness these same properties in order to create resilient machines. The nature of soft materials, however, presents considerable challenges to aspects of design, construction, and control -- and up until now, the vast majority of gaits for soft robots have been hand-designed through empirical trial-and-error. This manuscript describes an easy-to-assemble tensegrity-based soft robot capable of highly dynamic locomotive gaits and demonstrating structural and behavioral resilience in the face of physical damage. Enabling this is the use of a machine learning algorithm able to discover novel gaits with a minimal number of physical trials. These results lend further credence to soft-robotic approaches that seek to harness the interaction of complex material dynamics in order to generate a wealth of dynamical behaviors. version:1
arxiv-1702-03244 | $L_2$Boosting for Economic Applications | http://arxiv.org/abs/1702.03244 | id:1702.03244 author:Ye Luo, Martin Spindler category:stat.ML stat.ME  published:2017-02-10 summary:In the recent years more and more high-dimensional data sets, where the number of parameters $p$ is high compared to the number of observations $n$ or even larger, are available for applied researchers. Boosting algorithms represent one of the major advances in machine learning and statistics in recent years and are suitable for the analysis of such data sets. While Lasso has been applied very successfully for high-dimensional data sets in Economics, boosting has been underutilized in this field, although it has been proven very powerful in fields like Biostatistics and Pattern Recognition. We attribute this to missing theoretical results for boosting. The goal of this paper is to fill this gap and show that boosting is a competitive method for inference of a treatment effect or instrumental variable (IV) estimation in a high-dimensional setting. First, we present the $L_2$Boosting with componentwise least squares algorithm and variants which are tailored for regression problems which are the workhorse for most Econometric problems. Then we show how $L_2$Boosting can be used for estimation of treatment effects and IV estimation. We highlight the methods and illustrate them with simulations and empirical examples. For further results and technical details we refer to Luo and Spindler (2016, 2017) and to the online supplement of the paper. version:1
arxiv-1701-06264 | Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities | http://arxiv.org/abs/1701.06264 | id:1701.06264 author:Guo-Jun Qi category:cs.CV  published:2017-01-23 summary:*Updates to previous version* 1) we use the non-parametric characterization of optimal loss function to analyze how the vanishing gradient in training the generator of the classic GAN can be addressed by the LS-GAN; 2) better classification accuracy is reported; 3) we unify Wasserstein GAN (WGAN) under the same Lipschitz regularity to prove its consistency with the underlying data density. Abstract: In this paper, we present a novel Loss-Sensitive GAN (LS-GAN) that learns a loss function to separate generated samples from their real examples. An important property of the LS-GAN is it allows the generator to focus on improving poor data points that are far apart from real examples rather than wasting efforts on those samples that have already been well generated, and thus can improve the overall quality of generated samples. The theoretical analysis also shows that the LS-GAN can generate samples following the true data density. In particular, we present a regularity condition on the underlying data density, which allows us to use a class of Lipschitz losses and generators to model the LS-GAN. It relaxes the assumption that the classic GAN should have infinite modeling capacity to obtain the similar theoretical guarantee. Furthermore, we derive a non-parametric solution that characterizes the upper and lower bounds of the losses learned by the LS-GAN, both of which are piecewise linear and have non-vanishing gradient almost everywhere. Therefore, there should be sufficient gradient to update the generator of the LS-GAN even if the loss function is optimized, relieving the vanishing gradient problem in the classic GAN and making it easier to train the LS-GAN generator. We also generalize the unsupervised LS-GAN to a conditional model generating samples based on given conditions, and show its applications in both supervised and semi-supervised learning problems. version:2
arxiv-1702-01975 | Learning what matters - Sampling interesting patterns | http://arxiv.org/abs/1702.01975 | id:1702.01975 author:Vladimir Dzyuba, Matthijs van Leeuwen category:stat.ML cs.AI cs.DB  published:2017-02-07 summary:In the field of exploratory data mining, local structure in data can be described by patterns and discovered by mining algorithms. Although many solutions have been proposed to address the redundancy problems in pattern mining, most of them either provide succinct pattern sets or take the interests of the user into account-but not both. Consequently, the analyst has to invest substantial effort in identifying those patterns that are relevant to her specific interests and goals. To address this problem, we propose a novel approach that combines pattern sampling with interactive data mining. In particular, we introduce the LetSIP algorithm, which builds upon recent advances in 1) weighted sampling in SAT and 2) learning to rank in interactive pattern mining. Specifically, it exploits user feedback to directly learn the parameters of the sampling distribution that represents the user's interests. We compare the performance of the proposed algorithm to the state-of-the-art in interactive pattern mining by emulating the interests of a user. The resulting system allows efficient and interleaved learning and sampling, thus user-specific anytime data exploration. Finally, LetSIP demonstrates favourable trade-offs concerning both quality-diversity and exploitation-exploration when compared to existing methods. version:2
arxiv-1702-01238 | Large-scale Image Geo-Localization Using Dominant Sets | http://arxiv.org/abs/1702.01238 | id:1702.01238 author:Eyasu Zemene, Yonatan Tariku, Haroon Idrees, Andrea Prati, Marcello Pelillo, Mubarak Shah category:cs.CV  published:2017-02-04 summary:This paper presents a new approach for the challenging problem of geo-locating an image using image matching in a structured database of city-wide reference images with known GPS coordinates. We cast the geo-localization as a clustering problem on local image features. Akin to existing approaches on the problem, our framework builds on low-level features which allow partial matching between images. For each local feature in the query image, we find its approximate nearest neighbors in the reference set. Next, we cluster the features from reference images using Dominant Set clustering, which affords several advantages over existing approaches. First, it permits variable number of nodes in the cluster which we use to dynamically select the number of nearest neighbors (typically coming from multiple reference images) for each query feature based on its discrimination value. Second, as we also quantify in our experiments, this approach is several orders of magnitude faster than existing approaches. Thus, we obtain multiple clusters (different local maximizers) and obtain a robust final solution to the problem using multiple weak solutions through constrained Dominant Set clustering on global image features, where we enforce the constraint that the query image must be included in the cluster. This second level of clustering also bypasses heuristic approaches to voting and selecting the reference image that matches to the query. We evaluated the proposed framework on an existing dataset of 102k street view images as well as a new dataset of 300k images, and show that it outperforms the state-of-the-art by 20% and 7%, respectively, on the two datasets. version:2
arxiv-1702-03197 | Arabic Language Sentiment Analysis on Health Services | http://arxiv.org/abs/1702.03197 | id:1702.03197 author:Abdulaziz M. Alayba, Vasile Palade, Matthew England, Rahat Iqbal category:cs.CL cs.NE cs.SI I.2.7; I.2.6  published:2017-02-10 summary:The social media network phenomenon leads to a massive amount of valuable data that is available online and easy to access. Many users share images, videos, comments, reviews, news and opinions on different social networks sites, with Twitter being one of the most popular ones. Data collected from Twitter is highly unstructured, and extracting useful information from tweets is a challenging task. Twitter has a huge number of Arabic users who mostly post and write their tweets using the Arabic language. While there has been a lot of research on sentiment analysis in English, the amount of researches and datasets in Arabic language is limited. This paper introduces an Arabic language dataset which is about opinions on health services and has been collected from Twitter. The paper will first detail the process of collecting the data from Twitter and also the process of filtering, pre-processing and annotating the Arabic text in order to build a big sentiment analysis dataset in Arabic. Several Machine Learning algorithms (Naive Bayes, Support Vector Machine and Logistic Regression) alongside Deep and Convolutional Neural Networks were utilized in our experiments of sentiment analysis on our health dataset. version:1
arxiv-1702-03196 | Universal Semantic Parsing | http://arxiv.org/abs/1702.03196 | id:1702.03196 author:Siva Reddy, Oscar Täckström, Slav Petrov, Mark Steedman, Mirella Lapata category:cs.CL  published:2017-02-10 summary:Universal Dependencies (UD) provides a cross-linguistically uniform syntactic representation, with the aim of advancing multilingual applications of parsing and natural language understanding. Reddy et al. (2016) recently developed a semantic interface for (English) Stanford Dependencies, based on the lambda calculus. In this work, we introduce UDepLambda, a similar semantic interface for UD, which allows mapping natural language to logical forms in an almost language-independent framework. We evaluate our approach on semantic parsing for the task of question answering against Freebase. To facilitate multilingual evaluation, we provide German and Spanish translations of the WebQuestions and GraphQuestions datasets. Results show that UDepLambda outperforms strong baselines across languages and datasets. For English, it achieves the strongest result to date on GraphQuestions, with competitive results on WebQuestions. version:1
arxiv-1702-03192 | Improving the Performance of Fully Connected Neural Networks by Out-of-Place Matrix Transpose | http://arxiv.org/abs/1702.03192 | id:1702.03192 author:Shaohuai Shi, Pengfei Xu, Xiaowen Chu category:cs.DC cs.LG  published:2017-02-10 summary:Fully connected network has been widely used in deep learning, and its computation efficiency is highly benefited from the matrix multiplication algorithm with cuBLAS on GPU. However, We found that, there exist some drawbacks of cuBLAS in calculating matrix $\textbf{A}$ multiplies the transpose of matrix $\textbf{B}$ (i.e., NT operation). To reduce the impact of NT operation by cuBLAS, we exploit the out-of-place transpose of matrix $\textbf{B}$ to avoid using NT operation, and then we apply our method to Caffe, which is a popular deep learning tool. Our contribution is two-fold. First, we propose a naive method (TNN) and model-based method (MTNN) to increase the performance in calculating $\textbf{A}\times \textbf{B}^T$, and it achieves about 4.7 times performance enhancement in our tested cases on GTX1080 card. Second, we integrate MTNN method into Caffe to enhance the efficiency in training fully connected networks, which achieves about 70% speedup compared to the original Caffe in our configured fully connected networks on GTX1080 card. version:1
arxiv-1702-03176 | A clustering approach to heterogeneous change detection | http://arxiv.org/abs/1702.03176 | id:1702.03176 author:Luigi Tommaso Luppino, Stian Normann Anfinsen, Gabriele Moser, Robert Jenssen, Filippo Maria Bianchi, Sebastiano Serpico, Gregoire Mercier category:cs.CV  published:2017-02-10 summary:Change detection in heterogeneous multitemporal satellite images is a challenging and still not much studied topic in remote sensing and earth observation. This paper focuses on comparison of image pairs covering the same geographical area and acquired by two different sensors, one optical radiometer and one synthetic aperture radar, at two different times. We propose a clustering-based technique to detect changes, identified as clusters that split or merge in the different images. To evaluate potentials and limitations of our method, we perform experiments on real data. Preliminary results confirm the relationship between splits and merges of clusters and the occurrence of changes. However, it becomes evident that it is necessary to incorporate prior, ancillary, or application-specific information to improve the interpretation of clustering results and to identify unambiguously the areas of change. version:1
arxiv-1701-06547 | Adversarial Learning for Neural Dialogue Generation | http://arxiv.org/abs/1701.06547 | id:1701.06547 author:Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, Dan Jurafsky category:cs.CL  published:2017-01-23 summary:In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator---analagous to the human evaluator in the Turing test--- to distinguish between the human-generated dialogues and the machine-generated ones. The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues. In addition to adversarial training we describe a model for adversarial {\em evaluation} that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines. version:3
arxiv-1702-03121 | Modeling Semantic Expectation: Using Script Knowledge for Referent Prediction | http://arxiv.org/abs/1702.03121 | id:1702.03121 author:Ashutosh Modi, Ivan Titov, Vera Demberg, Asad Sayeed, Manfred Pinkal category:cs.CL cs.AI stat.ML  published:2017-02-10 summary:Recent research in psycholinguistics has provided increasing evidence that humans predict upcoming content. Prediction also affects perception and might be a key to robustness in human language processing. In this paper, we investigate the factors that affect human prediction by building a computational model that can predict upcoming discourse referents based on linguistic knowledge alone vs. linguistic knowledge jointly with common-sense knowledge in the form of scripts. We find that script knowledge significantly improves model estimates of human predictions. In a second study, we test the highly controversial hypothesis that predictability influences referring expression type but do not find evidence for such an effect. version:1
arxiv-1702-03118 | Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning | http://arxiv.org/abs/1702.03118 | id:1702.03118 author:Stefan Elfwing, Eiji Uchibe, Kenji Doya category:cs.LG  published:2017-02-10 summary:In recent years, neural networks have enjoyed a renaissance as function approximators in reinforcement learning. Two decades after Teasauro's TD-Gammon achieved near top-level human performance in backgammon, the deep reinforcement learning algorithm DQN (combining Q-learning with a deep neural network, experience replay, and a separate target network) achieved human-level performance in many Atari 2600 games. The purpose of this study is twofold. First, based on the expected energy restricted Boltzmann machine (EE-RBM), we propose two activation functions for neural network function approximation in reinforcement learning: the sigmoid-weighted linear (SiL) unit and its derivative function (SiLd1). The activation of the SiL unit is computed by the sigmoid function multiplied by its input, which is equal to the contribution to the output from one hidden unit in an EE-RBM. Second, we suggest that the more traditional approach of using on-policy learning with eligibility traces, instead of experience replay, and softmax action selection can be competitive with DQN, without the need for a separate target network. We validate our proposed approach by, first, achieving new state-of-the-art results in both stochastic SZ-Tetris and Tetris with a small 10x10 board, using TD($\lambda$) learning and shallow SiLd1 network agents, and, then, outperforming DQN in the Atari 2600 domain by using a deep Sarsa($\lambda$) agent with SiL and SiLd1 hidden units. version:1
arxiv-1702-03115 | Texture Characterization by Using Shape Co-occurrence Patterns | http://arxiv.org/abs/1702.03115 | id:1702.03115 author:Gui-Song Xia, Gang Liu, Xiang Bai, Liangpei Zhang category:cs.CV  published:2017-02-10 summary:Texture characterization is a key problem in image understanding and pattern recognition. In this paper, we present a flexible shape-based texture representation using shape co-occurrence patterns. More precisely, texture images are first represented by tree of shapes, each of which is associated with several geometrical and radiometric attributes. Then four typical kinds of shape co-occurrence patterns based on the hierarchical relationship of the shapes in the tree are learned as codewords. Three different coding methods are investigated to learn the codewords, with which, any given texture image can be encoded into a descriptive vector. In contrast with existing works, the proposed method not only inherits the strong ability to depict geometrical aspects of textures and the high robustness to variations of imaging conditions from the shape-based method, but also provides a flexible way to consider shape relationships and to compute high-order statistics on the tree. To our knowledge, this is the first time to use co-occurrence patterns of explicit shapes as a tool for texture analysis. Experiments on various texture datasets and scene datasets demonstrate the efficiency of the proposed method. version:1
arxiv-1702-03105 | Graph Fourier Transform with Negative Edges for Depth Image Coding | http://arxiv.org/abs/1702.03105 | id:1702.03105 author:Weng-Tai Su, Gene Cheung, Chia-Wen Lin category:cs.CV  published:2017-02-10 summary:Recent advent in graph signal processing (GSP) has led to the development of new graph-based transforms and wavelets for image / video coding, where the underlying graph describes inter-pixel correlations. In this paper, we develop a new transform called signed graph Fourier transform (SGFT), where the underlying graph G contains negative edges that describe anti-correlations between pixel pairs. Specifically, we first construct a one-state Markov process that models both inter-pixel correlations and anti-correlations. We then derive the corresponding precision matrix, and show that the loopy graph Laplacian matrix Q of a graph G with a negative edge and two self-loops at its end nodes is approximately equivalent. This proves that the eigenvectors of Q - called SGFT - approximates the optimal Karhunen-Lo`eve Transform (KLT). We show the importance of the self-loops in G to ensure Q is positive semi-definite. We prove that the first eigenvector of Q is piecewise constant (PWC), and thus can well approximate a piecewise smooth (PWS) signal like a depth image. Experimental results show that a block-based coding scheme based on SGFT outperforms a previous scheme using graph transforms with only positive edges for several depth images. version:1
arxiv-1702-03082 | UsingWord Embedding for Cross-Language Plagiarism Detection | http://arxiv.org/abs/1702.03082 | id:1702.03082 author:J. Ferrero, F. Agnes, L. Besacier, D. Schwab category:cs.CL  published:2017-02-10 summary:This paper proposes to use distributed representation of words (word embeddings) in cross-language textual similarity detection. The main contributions of this paper are the following: (a) we introduce new cross-language similarity detection methods based on distributed representation of words; (b) we combine the different methods proposed to verify their complementarity and finally obtain an overall F1 score of 89.15% for English-French similarity detection at chunk level (88.5% at sentence level) on a very challenging corpus. version:1
arxiv-1702-03070 | PCA in Data-Dependent Noise (Correlated-PCA): Nearly Optimal Finite Sample Guarantees | http://arxiv.org/abs/1702.03070 | id:1702.03070 author:Namrata Vaswani, Praneeth Narayanamurthy category:cs.IT math.IT stat.ML  published:2017-02-10 summary:We study Principal Component Analysis (PCA) in a setting where a part of the corrupting noise is data-dependent and, hence, the noise and the true data are correlated. Under a bounded-ness assumption on both the true data and noise, and a few assumptions on the data-noise correlation, we obtain a sample complexity bound for the most common PCA solution, singular value decomposition (SVD). This bound, which is within a logarithmic factor of the best achievable, significantly improves upon our bound from recent work (NIPS 2016) where we first studied this "correlated-PCA" problem. version:1
arxiv-1702-02555 | A Modified Construction for a Support Vector Classifier to Accommodate Class Imbalances | http://arxiv.org/abs/1702.02555 | id:1702.02555 author:Matt Parker, Colin Parker category:stat.ML cs.LG  published:2017-02-08 summary:Given a training set with binary classification, the Support Vector Machine identifies the hyperplane maximizing the margin between the two classes of training data. This general formulation is useful in that it can be applied without regard to variance differences between the classes. Ignoring these differences is not optimal, however, as the general SVM will give the class with lower variance an unjustifiably wide berth. This increases the chance of misclassification of the other class and results in an overall loss of predictive performance. An alternate construction is proposed in which the margins of the separating hyperplane are different for each class, each proportional to the standard deviation of its class along the direction perpendicular to the hyperplane. The construction agrees with the SVM in the case of equal class variances. This paper will then examine the impact to the dual representation of the modified constraint equations. version:2
arxiv-1702-03824 | Online People Tracking and Identification with RFID and Kinect | http://arxiv.org/abs/1702.03824 | id:1702.03824 author:Xinyu Li, Yanyi Zhang, Ivan Marsic, Randall S. Burd category:cs.CV  published:2017-02-10 summary:We introduce a novel, accurate and practical system for real-time people tracking and identification. We used a Kinect V2 sensor for tracking that generates a body skeleton for up to six people in the view. We perform identification using both Kinect and passive RFID, by first measuring the velocity vector of person's skeleton and of their RFID tag using the position of the RFID reader antennas as reference points and then finding the best match between skeletons and tags. We introduce a method for synchronizing Kinect data, which is captured regularly, with irregular or missing RFID data readouts. Our experiments show centimeter-level people tracking resolution with 80% average identification accuracy for up to six people in indoor environments, which meets the needs of many applications. Our system can preserve user privacy and work with different lighting. version:1
arxiv-1702-03056 | Sparse modeling approach to analytical continuation of imaginary-time quantum Monte Carlo data | http://arxiv.org/abs/1702.03056 | id:1702.03056 author:Junya Otsuki, Masayuki Ohzeki, Hiroshi Shinaoka, Kazuyoshi Yoshimi category:cond-mat.str-el cond-mat.stat-mech stat.ML  published:2017-02-10 summary:A new approach of solving the ill-conditioned inverse problem for analytical continuation is proposed. The root of the problem lies in the fact that even tiny noise of imaginary-time input data has a serious impact on the inferred real-frequency spectra. By means of a modern regularization technique, we eliminate redundant degrees of freedom that essentially carry the noise, leaving only relevant information unaffected by the noise. The resultant spectrum is represented with minimal bases and thus a stable analytical continuation is achieved. This framework further provides a tool to clarify to what extent the fine structure of the true spectral function can be reconstructed from imaginary-time data. version:1
arxiv-1702-03054 | Compression of imaginary-time data using intermediate representation of analytical continuation | http://arxiv.org/abs/1702.03054 | id:1702.03054 author:Hiroshi Shinaoka, Junya Otsuki, Masayuki Ohzeki, Kazuyoshi Yoshimi category:cond-mat.str-el cond-mat.stat-mech stat.ML  published:2017-02-10 summary:New model-independent compact representations of imaginary-time data are presented in terms of the intermediate representation (IR) of analytical continuation. This is motivated by a recent numerical finding by the authors [J. Otsuki et al. (2017)]. We demonstrate the efficiency of the present method through continuous-time quantum Monte Carlo calculations of an Anderson impurity model. We find that the IR yields a significantly compact form of various types of correlation functions. The present framework will provide general ways to boost the power of cutting-edge diagrammatic/quantum Monte Carlo treatments of many-body systems. version:1
arxiv-1702-03044 | Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights | http://arxiv.org/abs/1702.03044 | id:1702.03044 author:Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, Yurong Chen category:cs.CV cs.AI cs.NE  published:2017-02-10 summary:This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization, our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. The code will be made publicly available. version:1
arxiv-1702-03041 | Reconstruction for Feature Disentanglement in Pose-invariant Face Recognition | http://arxiv.org/abs/1702.03041 | id:1702.03041 author:Xi Peng, Xiang Yu, Kihyuk Sohn, Dimitris Metaxas, Manmohan Chandraker category:cs.CV  published:2017-02-10 summary:Deep neural networks (DNNs) trained on large-scale datasets have recently achieved impressive improvements in face recognition. But a persistent challenge remains to develop methods capable of handling large pose variations that are relatively under-represented in training data. This paper presents a method for learning a feature representation that is invariant to pose, without requiring extensive pose coverage in training data. We first propose to use a synthesis network for generating non-frontal views from a single frontal image, in order to increase the diversity of training data while preserving accurate facial details that are critical for identity discrimination. Our next contribution is a multi-source multi-task DNN that seeks a rich embedding representing identity information, as well as information such as pose and landmark locations. Finally, we propose a Siamese network to explicitly disentangle identity and pose, by demanding alignment between the feature reconstructions through various combinations of identity and pose features obtained from two images of the same subject. Experiments on face datasets in both controlled and wild scenarios, such as MultiPIE, LFW and 300WLP, show that our method consistently outperforms the state-of-the-art, especially on images with large head pose variations. version:1
arxiv-1702-03040 | Following the Leader and Fast Rates in Linear Prediction: Curved Constraint Sets and Other Regularities | http://arxiv.org/abs/1702.03040 | id:1702.03040 author:Ruitong Huang, Tor Lattimore, András György, Csaba Szepesvári category:cs.LG  published:2017-02-10 summary:The follow the leader (FTL) algorithm, perhaps the simplest of all online learning algorithms, is known to perform well when the loss functions it is used on are convex and positively curved. In this paper we ask whether there are other "lucky" settings when FTL achieves sublinear, "small" regret. In particular, we study the fundamental problem of linear prediction over a non-empty convex, compact domain. Amongst other results, we prove that the curvature of the boundary of the domain can act as if the losses were curved: In this case, we prove that as long as the mean of the loss vectors have positive lengths bounded away from zero, FTL enjoys a logarithmic growth rate of regret, while, e.g., for polytope domains and stochastic data it enjoys finite expected regret. Building on a previously known meta-algorithm, we also get an algorithm that simultaneously enjoys the worst-case guarantees and the bound available for FTL. version:1
arxiv-1702-03037 | Multi-agent Reinforcement Learning in Sequential Social Dilemmas | http://arxiv.org/abs/1702.03037 | id:1702.03037 author:Joel Z. Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, Thore Graepel category:cs.MA cs.AI cs.GT cs.LG  published:2017-02-10 summary:Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation. version:1
arxiv-1702-03033 | Local System Voting Feature for Machine Translation System Combination | http://arxiv.org/abs/1702.03033 | id:1702.03033 author:Markus Freitag, Jan-Thorsten Peter, Stephan Peitz, Minwei Feng, Hermann Ney category:cs.CL  published:2017-02-10 summary:In this paper, we enhance the traditional confusion network system combination approach with an additional model trained by a neural network. This work is motivated by the fact that the commonly used binary system voting models only assign each input system a global weight which is responsible for the global impact of each input system on all translations. This prevents individual systems with low system weights from having influence on the system combination output, although in some situations this could be helpful. Further, words which have only been seen by one or few systems rarely have a chance of being present in the combined output. We train a local system voting model by a neural network which is based on the words themselves and the combinatorial occurrences of the different system outputs. This gives system combination the option to prefer other systems at different word positions even for the same sentence. version:1
arxiv-1701-04245 | Learning Traffic as Images: A Deep Convolution Neural Network for Large-scale Transportation Network Speed Prediction | http://arxiv.org/abs/1701.04245 | id:1701.04245 author:Xiaolei Ma, Zhuang Dai, Zhengbing He, Jihui Na, Yong Wang, Yunpeng Wang category:cs.LG stat.ML  published:2017-01-16 summary:This paper proposes a convolution neural network (CNN)-based method that learns traffic as images and predicts large-scale, network-wide traffic speed with high accuracy. Spatiotemporal traffic dynamics is converted to images describing the time and space relations of traffic flow via a two-dimensional time-space matrix. CNN is applied to the image following two consecutive steps: abstract traffic feature extraction and network-wide traffic speed prediction. The effectiveness of the proposed method is evaluated by taking two real-world transportation networks, the second ring road and north-east transportation network in Beijing, as examples, and comparing the method with four prevailing algorithms, namely, ordinary least squares, k-nearest neighbors, artificial neural network, and random forest. The results show that the proposed method outperforms the four algorithms by an average accuracy improvement of 27.96% within acceptable execution time. The CNN can train the model in reasonable time and thus are suitable for large-scale transportation networks. version:3
arxiv-1702-03023 | A New Rank Constraint on Multi-view Fundamental Matrices, and its Application to Camera Location Recovery | http://arxiv.org/abs/1702.03023 | id:1702.03023 author:Soumyadip Sengupta, Tal Amir, Meirav Galun, Tom Goldstein, David W. Jacobs, Amit Singer, Ronen Basri category:cs.CV  published:2017-02-10 summary:Accurate estimation of camera matrices is an important step in structure from motion algorithms. In this paper we introduce a novel rank constraint on collections of fundamental matrices in multi-view settings. We show that in general, with the selection of proper scale factors, a matrix formed by stacking fundamental matrices between pairs of images has rank 6. Moreover, this matrix forms the symmetric part of a rank 3 matrix whose factors relate directly to the corresponding camera matrices. We use this new characterization to produce better estimations of fundamental matrices by optimizing an L1-cost function using Iterative Re-weighted Least Squares and Alternate Direction Method of Multiplier. We further show that this procedure can improve the recovery of camera locations, particularly in multi-view settings in which fewer images are available. version:1
arxiv-1702-03006 | Multi-step Off-policy Learning Without Importance Sampling Ratios | http://arxiv.org/abs/1702.03006 | id:1702.03006 author:Ashique Rupam Mahmood, Huizhen Yu, Richard S. Sutton category:cs.LG  published:2017-02-09 summary:To estimate the value functions of policies from exploratory data, most model-free off-policy algorithms rely on importance sampling, where the use of importance sampling ratios often leads to estimates with severe variance. It is thus desirable to learn off-policy without using the ratios. However, such an algorithm does not exist for multi-step learning with function approximation. In this paper, we introduce the first such algorithm based on temporal-difference (TD) learning updates. We show that an explicit use of importance sampling ratios can be eliminated by varying the amount of bootstrapping in TD updates in an action-dependent manner. Our new algorithm achieves stability using a two-timescale gradient-based TD update. A prior algorithm based on lookup table representation called Tree Backup can also be retrieved using action-dependent bootstrapping, becoming a special case of our algorithm. In two challenging off-policy tasks, we demonstrate that our algorithm is stable, effectively avoids the large variance issue, and can perform substantially better than its state-of-the-art counterpart. version:1
arxiv-1702-03000 | A large comparison of feature-based approaches for buried target classification in forward-looking ground-penetrating radar | http://arxiv.org/abs/1702.03000 | id:1702.03000 author:Joseph A. Camilo, Leslie M. Collins, Jordan M. Malof category:cs.CV  published:2017-02-09 summary:Forward-looking ground-penetrating radar (FLGPR) has recently been investigated as a remote sensing modality for buried target detection (e.g., landmines). In this context, raw FLGPR data is beamformed into images and then computerized algorithms are applied to automatically detect subsurface buried targets. Most existing algorithms are supervised, meaning they are trained to discriminate between labeled target and non-target imagery, usually based on features extracted from the imagery. A large number of features have been proposed for this purpose, however thus far it is unclear which are the most effective. The first goal of this work is to provide a comprehensive comparison of detection performance using existing features on a large collection of FLGPR data. Fusion of the decisions resulting from processing each feature is also considered. The second goal of this work is to investigate two modern feature learning approaches from the object recognition literature: the bag-of-visual-words and the Fisher vector for FLGPR processing. The results indicate that the new feature learning approaches outperform existing methods. Results also show that fusion between existing features and new features yields little additional performance improvements. version:1
arxiv-1702-02982 | Fixing an error in Caponnetto and de Vito (2007) | http://arxiv.org/abs/1702.02982 | id:1702.02982 author:Dougal J. Sutherland category:stat.ML cs.LG math.ST stat.TH  published:2017-02-09 summary:The seminal paper of Caponnetto and de Vito (2007) provides minimax-optimal rates for kernel ridge regression in a very general setting. Its proof, however, contains an error in its bound on the effective dimensionality. In this note, we explain the mistake, provide a correct bound, and show that the main theorem remains true. version:1
arxiv-1701-07422 | An Iterative Algorithm for Sparse Recovery of Missing Image Samples Using a New Similarity Index | http://arxiv.org/abs/1701.07422 | id:1701.07422 author:Amirhossein Javaheri, Hadi Zayyani, Farokh Marvasti category:cs.LG stat.ML  published:2017-01-25 summary:This paper investigates the problem of recovering missing samples using methods based on sparse representation adapted especially for image signals. Instead of $l_2$-norm or Mean Square Error (MSE), a new perceptual quality measure is used as the similarity criterion between the original and the reconstructed images. The proposed metric called Convex SIMilarity (CSIM) index is a modified version of the Structural SIMilarity (SSIM) index which despite its predecessor, is convex and uni-modal. We also propose an iterative sparse recovery method based on a constrained $l_1$-norm minimization problem involving CSIM as the fidelity criterion. This optimization problem which is adopted for missing sample recovery of images is efficiently solved via an algorithm based on Alternating Direction Method of Multipliers (ADMM). Simulation results show the performance of the new similarity index as well as the proposed algorithm for missing sample recovery of test images. version:2
arxiv-1702-02925 | EAC-Net: A Region-based Deep Enhancing and Cropping Approach for Facial Action Unit Detection | http://arxiv.org/abs/1702.02925 | id:1702.02925 author:Wei Li, Farnaz Abtahi, Zhigang Zhu, Lijun Yin category:cs.CV  published:2017-02-09 summary:In this paper, we propose a deep learning based approach for facial action unit detection by enhancing and cropping the regions of interest. The approach is implemented by adding two novel nets (layers): the enhancing layers and the cropping layers, to a pretrained CNN model. For the enhancing layers, we designed an attention map based on facial landmark features and applied it to a pretrained neural network to conduct enhanced learning (The E-Net). For the cropping layers, we crop facial regions around the detected landmarks and design convolutional layers to learn deeper features for each facial region (C-Net). We then fuse the E-Net and the C-Net to obtain our Enhancing and Cropping (EAC) Net, which can learn both feature enhancing and region cropping functions. Our approach shows significant improvement in performance compared to the state-of-the-art methods applied to BP4D and DISFA AU datasets. version:1
arxiv-1702-02914 | Spatial Filtering for EEG-Based Regression Problems in Brain-Computer Interface (BCI) | http://arxiv.org/abs/1702.02914 | id:1702.02914 author:Dongrui Wu, Jung-Tai King, Chun-Hsiang Chuang, Chin-Teng Lin, Tzyy-Ping Jung category:cs.LG cs.HC  published:2017-02-09 summary:Electroencephalogram (EEG) signals are frequently used in brain-computer interfaces (BCIs), but they are easily contaminated by artifacts and noises, so preprocessing must be done before they are fed into a machine learning algorithm for classification or regression. Spatial filters have been widely used to increase the signal-to-noise ratio of EEG for BCI classification problems, but their applications in BCI regression problems have been very limited. This paper proposes two common spatial pattern (CSP) filters for EEG-based regression problems in BCI, which are extended from the CSP filter for classification, by making use of fuzzy sets. Experimental results on EEG-based response speed estimation from a large-scale study, which collected 143 sessions of sustained-attention psychomotor vigilance task data from 17 subjects during a 5-month period, demonstrate that the two proposed spatial filters can significantly increase the EEG signal quality. When used in LASSO and k-nearest neighbors regression for user response speed estimation, the spatial filters can reduce the root mean square estimation error by 10.02-19.77%, and at the same time increase the correlation to the true response speed by 19.39-86.47%. version:1
arxiv-1702-02906 | Switching EEG Headsets Made Easy: Reducing Offline Calibration Effort Using Active Weighted Adaptation Regularization | http://arxiv.org/abs/1702.02906 | id:1702.02906 author:Dongrui Wu, Vernon J. Lawhern, W. David Hairston, Brent J. Lance category:cs.LG cs.HC  published:2017-02-09 summary:Electroencephalography (EEG) headsets are the most commonly used sensing devices for Brain-Computer Interface. In real-world applications, there are advantages to extrapolating data from one user session to another. However, these advantages are limited if the data arise from different hardware systems, which often vary between application spaces. Currently, this creates a need to recalibrate classifiers, which negatively affects people's interest in using such systems. In this paper, we employ active weighted adaptation regularization (AwAR), which integrates weighted adaptation regularization (wAR) and active learning, to expedite the calibration process. wAR makes use of labeled data from the previous headset and handles class-imbalance, and active learning selects the most informative samples from the new headset to label. Experiments on single-trial event-related potential classification show that AwAR can significantly increase the classification accuracy, given the same number of labeled samples from the new headset. In other words, AwAR can effectively reduce the number of labeled samples required from the new headset, given a desired classification accuracy, suggesting value in collating data for use in wide scale transfer-learning applications. version:1
arxiv-1702-02901 | Driver Drowsiness Estimation from EEG Signals Using Online Weighted Adaptation Regularization for Regression (OwARR) | http://arxiv.org/abs/1702.02901 | id:1702.02901 author:Dongrui Wu, Vernon J. Lawhern, Stephen Gordon, Brent J. Lance, Chin-Teng Lin category:cs.LG cs.HC  published:2017-02-09 summary:One big challenge that hinders the transition of brain-computer interfaces (BCIs) from laboratory settings to real-life applications is the availability of high-performance and robust learning algorithms that can effectively handle individual differences, i.e., algorithms that can be applied to a new subject with zero or very little subject-specific calibration data. Transfer learning and domain adaptation have been extensively used for this purpose. However, most previous works focused on classification problems. This paper considers an important regression problem in BCI, namely, online driver drowsiness estimation from EEG signals. By integrating fuzzy sets with domain adaptation, we propose a novel online weighted adaptation regularization for regression (OwARR) algorithm to reduce the amount of subject-specific calibration data, and also a source domain selection (SDS) approach to save about half of the computational cost of OwARR. Using a simulated driving dataset with 15 subjects, we show that OwARR and OwARR-SDS can achieve significantly smaller estimation errors than several other approaches. We also provide comprehensive analyses on the robustness of OwARR and OwARR-SDS. version:1
arxiv-1702-02897 | Online and Offline Domain Adaptation for Reducing BCI Calibration Effort | http://arxiv.org/abs/1702.02897 | id:1702.02897 author:Dongrui Wu category:cs.LG cs.HC  published:2017-02-09 summary:Many real-world brain-computer interface (BCI) applications rely on single-trial classification of event-related potentials (ERPs) in EEG signals. However, because different subjects have different neural responses to even the same stimulus, it is very difficult to build a generic ERP classifier whose parameters fit all subjects. The classifier needs to be calibrated for each individual subject, using some labeled subject-specific data. This paper proposes both online and offline weighted adaptation regularization (wAR) algorithms to reduce this calibration effort, i.e., to minimize the amount of labeled subject-specific EEG data required in BCI calibration, and hence to increase the utility of the BCI system. We demonstrate using a visually-evoked potential oddball task and three different EEG headsets that both online and offline wAR algorithms significantly outperform several other algorithms. Moreover, through source domain selection, we can reduce their computational cost by about 50%, making them more suitable for real-time applications. version:1
arxiv-1702-02896 | Efficient Policy Learning | http://arxiv.org/abs/1702.02896 | id:1702.02896 author:Susan Athey, Stefan Wager category:math.ST cs.LG stat.ML stat.TH  published:2017-02-09 summary:There has been considerable interest across several fields in methods that reduce the problem of learning good treatment assignment policies to the problem of accurate policy evaluation. Given a class of candidate policies, these methods first effectively evaluate each policy individually, and then learn a policy by optimizing the estimated value function; such approaches are guaranteed to be risk-consistent whenever the policy value estimates are uniformly consistent. However, despite the wealth of proposed methods, the literature remains largely silent on questions of statistical efficiency: there are only limited results characterizing which policy evaluation strategies lead to better learned policies than others, or what the optimal policy evaluation strategies are. In this paper, we build on classical results in semiparametric efficiency theory to develop quasi-optimal methods for policy learning; in particular, we propose a class of policy value estimators that, when optimized, yield regret bounds for the learned policy that scale with the semiparametric efficient variance for policy evaluation. On a practical level, our result suggests new methods for policy learning motivated by semiparametric efficiency theory. version:1
arxiv-1702-01460 | A scikit-based Python environment for performing multi-label classification | http://arxiv.org/abs/1702.01460 | id:1702.01460 author:Piotr Szymański, Tomasz Kajdanowicz category:cs.LG cs.MS  published:2017-02-05 summary:Scikit-multilearn is a Python library for performing multi-label classification. The library is compatible with the scikit/scipy ecosystem and uses sparse matrices for all internal operations. It provides native Python implementations of popular multi-label classification methods alongside novel framework for label space partitioning and division. It includes graph-based community detection methods that utilize the powerful igraph library for extracting label dependency information. In addition its code is well test covered and follows PEP8. Source code and documentation can be downloaded from http://scikit.ml and also via pip. The library follows scikit's BSD licencing scheme. version:3
arxiv-1701-08142 | Modelling Ranking Data with the Wallenius Distribution | http://arxiv.org/abs/1701.08142 | id:1701.08142 author:Clara Grazian, Fabrizio Leisen, Brunero Liseo category:stat.ME stat.AP stat.CO stat.ML  published:2017-01-27 summary:Ranking datasets is useful when statements on the order of observations are more important than the magnitude of their differences and little is known about the underlying distribution of the data. The Wallenius distribution is a generalisation of the Hypergeometric distribution where weights are assigned to balls of different colours. This naturally defines a model for ranking categories which can be used for classification purposes. In this paper, we adopt an approximate Bayesian computational (ABC) approach since, in general, the resulting likelihood is not analytically available. We illustrate the performance of the estimation procedure on simulated datasets. Finally, we use the new model for analysing two datasets about movies ratings and Italian academic statisticians' journals preferences. The latter is a novel dataset collected by the authors. version:2
arxiv-1702-02849 | Coordinated Online Learning With Applications to Learning User Preferences | http://arxiv.org/abs/1702.02849 | id:1702.02849 author:Christoph Hirnschall, Adish Singla, Sebastian Tschiatschek, Andreas Krause category:cs.LG stat.ML  published:2017-02-09 summary:We study an online multi-task learning setting, in which instances of related tasks arrive sequentially, and are handled by task-specific online learners. We consider an algorithmic framework to model the relationship of these tasks via a set of convex constraints. To exploit this relationship, we design a novel algorithm -- COOL -- for coordinating the individual online learners: Our key idea is to coordinate their parameters via weighted projections onto a convex set. By adjusting the rate and accuracy of the projection, the COOL algorithm allows for a trade-off between the benefit of coordination and the required computation/communication. We derive regret bounds for our approach and analyze how they are influenced by these trade-off factors. We apply our results on the application of learning users' preferences on the Airbnb marketplace with the goal of incentivizing users to explore under-reviewed apartments. version:1
arxiv-1702-02828 | Minimax Lower Bounds for Ridge Combinations Including Neural Nets | http://arxiv.org/abs/1702.02828 | id:1702.02828 author:Jason M. Klusowski, Andrew R. Barron category:stat.ML cs.LG 62J02  62G08  68T05  published:2017-02-09 summary:Estimation of functions of $ d $ variables is considered using ridge combinations of the form $ \textstyle\sum_{k=1}^m c_{1,k} \phi(\textstyle\sum_{j=1}^d c_{0,j,k}x_j-b_k) $ where the activation function $ \phi $ is a function with bounded value and derivative. These include single-hidden layer neural networks, polynomials, and sinusoidal models. From a sample of size $ n $ of possibly noisy values at random sites $ X \in B = [-1,1]^d $, the minimax mean square error is examined for functions in the closure of the $ \ell_1 $ hull of ridge functions with activation $ \phi $. It is shown to be of order $ d/n $ to a fractional power (when $ d $ is of smaller order than $ n $), and to be of order $ (\log d)/n $ to a fractional power (when $ d $ is of larger order than $ n $). Dependence on constraints $ v_0 $ and $ v_1 $ on the $ \ell_1 $ norms of inner parameter $ c_0 $ and outer parameter $ c_1 $, respectively, is also examined. Also, lower and upper bounds on the fractional power are given. The heart of the analysis is development of information-theoretic packing numbers for these classes of functions. version:1
arxiv-1702-02817 | Graph Based Relational Features for Collective Classification | http://arxiv.org/abs/1702.02817 | id:1702.02817 author:Immanuel Bayer, Uwe Nagel, Steffen Rendle category:cs.IR cs.AI cs.LG  published:2017-02-09 summary:Statistical Relational Learning (SRL) methods have shown that classification accuracy can be improved by integrating relations between samples. Techniques such as iterative classification or relaxation labeling achieve this by propagating information between related samples during the inference process. When only a few samples are labeled and connections between samples are sparse, collective inference methods have shown large improvements over standard feature-based ML methods. However, in contrast to feature based ML, collective inference methods require complex inference procedures and often depend on the strong assumption of label consistency among related samples. In this paper, we introduce new relational features for standard ML methods by extracting information from direct and indirect relations. We show empirically on three standard benchmark datasets that our relational features yield results comparable to collective inference methods. Finally we show that our proposal outperforms these methods when additional information is available. version:1
arxiv-1702-02805 | Attribute-controlled face photo synthesis from simple line drawing | http://arxiv.org/abs/1702.02805 | id:1702.02805 author:Qi Guo, Ce Zhu, Zhiqiang Xia, Zhengtao Wang, Yipeng Liu category:cs.CV  published:2017-02-09 summary:Face photo synthesis from simple line drawing is a one-to-many task as simple line drawing merely contains the contour of human face. Previous exemplar-based methods are over-dependent on the datasets and are hard to generalize to complicated natural scenes. Recently, several works utilize deep neural networks to increase the generalization, but they are still limited in the controllability of the users. In this paper, we propose a deep generative model to synthesize face photo from simple line drawing controlled by face attributes such as hair color and complexion. In order to maximize the controllability of face attributes, an attribute-disentangled variational auto-encoder (AD-VAE) is firstly introduced to learn latent representations disentangled with respect to specified attributes. Then we conduct photo synthesis from simple line drawing based on AD-VAE. Experiments show that our model can well disentangle the variations of attributes from other variations of face photos and synthesize detailed photorealistic face images with desired attributes. Regarding background and illumination as the style and human face as the content, we can also synthesize face photos with the target style of a style photo. version:1
arxiv-1702-02779 | On-the-Fly Adaptation of Regression Forests for Online Camera Relocalisation | http://arxiv.org/abs/1702.02779 | id:1702.02779 author:Tommaso Cavallari, Stuart Golodetz, Nicholas A. Lord, Julien Valentin, Luigi Di Stefano, Philip H. S. Torr category:cs.CV  published:2017-02-09 summary:Camera relocalisation is a key problem in computer vision, with applications as diverse as simultaneous localisation and mapping, virtual/augmented reality and navigation. Common techniques either match the current image against keyframes with known poses coming from a tracker, or establish 2D-to-3D correspondences between keypoints in the current image and points in the scene in order to estimate the camera pose. Recently, regression forests have become a popular alternative to establish such correspondences. They achieve accurate results, but must be trained offline on the target scene, preventing relocalisation in new environments. In this paper, we show how to circumvent this limitation by adapting a pre-trained forest to a new scene on the fly. Our adapted forests achieve relocalisation performance that is on par with that of offline forests, and our approach runs in under 150ms, making it desirable for real-time systems that require online relocalisation. version:1
arxiv-1701-02854 | Decoding as Continuous Optimization in Neural Machine Translation | http://arxiv.org/abs/1701.02854 | id:1701.02854 author:Cong Duy Vu Hoang, Gholamreza Haffari, Trevor Cohn category:cs.CL cs.AI  published:2017-01-11 summary:We propose a novel decoding approach for neural machine translation (NMT) based on continuous optimisation. The resulting optimisation problem is then tackled using constrained gradient optimisation. Our powerful decoding framework, enables decoding intractable models such as the intersection of left-to-right and right-to-left (bidirectional) as well as source-to-target and target-to-source (bilingual) NMT models. Our empirical results show that our decoding framework is effective, and leads to substantial improvements in translations generated from the intersected models where the typical greedy or beam search is infeasible. version:3
arxiv-1702-02363 | Automatically Annotated Turkish Corpus for Named Entity Recognition and Text Categorization using Large-Scale Gazetteers | http://arxiv.org/abs/1702.02363 | id:1702.02363 author:H. Bahadir Sahin, Caglar Tirkaz, Eray Yildiz, Mustafa Tolga Eren, Ozan Sonmez category:cs.CL  published:2017-02-08 summary:Turkish Wikipedia Named-Entity Recognition and Text Categorization (TWNERTC) dataset is a collection of automatically categorized and annotated sentences obtained from Wikipedia. We constructed large-scale gazetteers by using a graph crawler algorithm to extract relevant entity and domain information from a semantic knowledge base, Freebase. The constructed gazetteers contains approximately 300K entities with thousands of fine-grained entity types under 77 different domains. Since automated processes are prone to ambiguity, we also introduce two new content specific noise reduction methodologies. Moreover, we map fine-grained entity types to the equivalent four coarse-grained types: person, loc, org, misc. Eventually, we construct six different dataset versions and evaluate the quality of annotations by comparing ground truths from human annotators. We make these datasets publicly available to support studies on Turkish named-entity recognition (NER) and text categorization (TC). version:2
